[{"authors":null,"categories":null,"content":"Adi Sarid is a partner and head of Operations Research Department at the Sarid Research Institute LTD.\nAdi has a PhD in Operations Research from the department of Industrial Engineering in Tel-Aviv university, an MSc in Operations Research from Tel-Aviv university, and a BA in Mathematics Statistics and Operations Research from the Technion.\nAdi is also a certified RStudio instructor, teaching tidyverse, shiny, statistics, and data science.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f8818db5ae9279c4270c8f0d28984ccf","permalink":"https://adisarid.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Adi Sarid is a partner and head of Operations Research Department at the Sarid Research Institute LTD.\nAdi has a PhD in Operations Research from the department of Industrial Engineering in Tel-Aviv university, an MSc in Operations Research from Tel-Aviv university, and a BA in Mathematics Statistics and Operations Research from the Technion.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor admin FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://adisarid.github.io/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://adisarid.github.io/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://adisarid.github.io/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://adisarid.github.io/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://adisarid.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":[],"categories":["R"],"content":"  Here is a detective story about some data science forensics, and there is a villain too.\nBackground As a partner at a market research firm, I get involved in quite a lot of surveys. Some of them (such as voice of customer surveys/customer satisfaction) include sending a personal email invitation with ‚Äúan invite link‚Äù.\nLately, we‚Äôve started noticing a weird phenomenon: a lot of the links are opened (the email invitation link is clicked) but left unanswered. Also, we see a lot of cases where expect responses from a specific location, but see that originate all over Europe.\nWe started asking ourselves, are we witnessing an abuse of our surveys?\n Validating Respondents One of my team members noticed some irregularities during our routine QA validations: responses were originating from unexpected remote locations all over Europe. A lot of duplicated IPs, as if a lot of people were suddenly using a VPN or something like that. Using a whois service I understood that the links are being opened by Microsoft owned IPs.\nMy first thought was that they started using some bot which examines the survey link within the email. Why exactly? at that point I hypothesized it was some kind of caching going on, probably for speed improvements (but that‚Äôs not the deal).\nSince no false information was being entered in these ‚Äúpartial surveys‚Äù, no actual harm was done by this bot. Except that it‚Äôs not entirely accurate, you see - in some surveys, the practice is to embed the first question of the survey within the invitation email. That way the first click of the recipient includes the answer to the first question, e.g., a ‚ÄúNet Promoters Score‚Äù (NPS) or general satisfaction question. It helps with response rates, but unfortunately the bot started clicking the links randomely and entering grabage data.\n A bot with random thoughts Trying to analyze when the problem started affecting my clients, I created a chart similar to following, which compares partial answers (which are affected by the bot) versus complete answers (the bot doesn‚Äôt complete the survey entirely, so complete answers are real people). I‚Äôve added an illustration (I‚Äôm not using real data here so not to expose actual client‚Äôs data).\nlibrary(tidyverse) responses \u0026lt;- read_csv(\u0026quot;example_data.csv\u0026quot;) %\u0026gt;% mutate(response_time = factor(response_time, c(\u0026quot;Before 2021-06-01\u0026quot;, \u0026quot;After 2021-06-01\u0026quot;))) ggplot(responses, aes(x = type, fill = factor(rating))) + geom_bar(position = \u0026quot;fill\u0026quot;) + facet_wrap(~response_time) + scale_fill_brewer(palette = \u0026quot;RdYlGn\u0026quot;) + guides(fill = guide_legend(\u0026quot;Rating\u0026quot;)) + theme_bw() + scale_y_continuous(labels = scales::percent) + ylab(\u0026quot;Respondents [%]\u0026quot;) + xlab(\u0026quot;\u0026quot;) + ggtitle(\u0026quot;Distribution of complete vs. partial responses,\\nbefore/after June 21\u0026quot;)  ","date":1627603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627626235,"objectID":"5e775177bba7422da35e3887e21bf688","permalink":"https://adisarid.github.io/post/data-science-forensics/","publishdate":"2021-07-30T00:00:00Z","relpermalink":"/post/data-science-forensics/","section":"post","summary":"Here is a detective story about some data science forensics, and there is a villain too.\nBackground As a partner at a market research firm, I get involved in quite a lot of surveys.","tags":["Safe Links","Spam","Microsoft","Alchemer"],"title":"A data science detective story (and there's a villain too)","type":"post"},{"authors":null,"categories":["R"],"content":"  Error catching can be hard to catch at times (no pun intended). If you‚Äôre not used to error handling, this short post might help you do it elegantly.\nThere are many posts about error handling in R (and in fact the examples in the purrr package documentation are not bad either). In this sense, this post is not original.\nHowever, I do demonstrate two approaches: both the base-R approach (tryCatch) and the purrr approach (safely and possibly). The post contains a concise summary of the two methods, with a very simple example.\nIn this post I‚Äôm assuming you‚Äôre familiar with the basic concepts of functional programming.\nA simple example for a function with errors Let‚Äôs analyze a very simple function which divides number by 2. The function should return an error if its input is not an actual number, otherwise it will return number/2. This function looks like this:\ndivide2 \u0026lt;- function(number){ # make sure the input is numeric (otherwise yield an error) if (!is.numeric(number)) { stop(paste(number, \u0026quot;is not a number!\u0026quot;)) } # everything is fine, return the result number/2 } divide2(10) ## [1] 5 divide2(pi) ## [1] 1.570796 But trying a string will yield an error:\ndivide2(\u0026quot;foobar\u0026quot;) Error in divide2(\u0026quot;foobar\u0026quot;) : foobar is not a number!  Where is the problem? What happens if we have a dataframe (or a list, or any other object for that matter) and we want to try to divide numbers within that object? Invalid values might crash our attempt completely.\nFor example, this loops through two values (10 and \\(\\pi\\)) and yields their division by 2.\nlibrary(purrr) map(list(10, pi), divide2) ## [[1]] ## [1] 5 ## ## [[2]] ## [1] 1.570796 But the next version fails completely, because it tries to loop through ‚Äúfoobar‚Äù which cannot be divided.\nmap(list(\u0026quot;foobar\u0026quot;, 10, pi), divide2) Error in .f(.x[[i]], ...) : foobar is not a number! No matter where we place the invalid value ‚Äúfoobar‚Äù, it will fail our code completely, and we get nothing.\n The solution: a safely/possibly wrapper Fortunately, there are very simple wrappers which can help us handle the errors elegantly. These are the two functions from the purrr package: safely and possibly.\nWe‚Äôll first demonstrate the simpler version, possibly. It allows us to replace errors with a chosen value. Since we expect a number, let‚Äôs replace errors with NA_real_ (which is like saying an unknown value which is a real number).\npossibly_divide2 \u0026lt;- possibly(divide2, otherwise = NA_real_, quiet = TRUE) The quiet = TRUE argument is just so errors will not be printed during the loop. Now we are ready to try our error safe variation.\npossibly_out \u0026lt;- map(list(\u0026quot;foobar\u0026quot;, 10, pi), possibly_divide2) possibly_out ## [[1]] ## [1] NA ## ## [[2]] ## [1] 5 ## ## [[3]] ## [1] 1.570796 We can also use unlist to turn the output into a simple vector, i.e.¬†unlist(possibly_out) will yield the vector NA, 5, 1.5707963.\nThe safely variation has some more strength into it, since it also provides the error messages. The output is slightly more complex though.\nsafely_divide2 \u0026lt;- safely(.f = divide2, otherwise = NA_real_, quiet = T) safely_out \u0026lt;- map(list(\u0026quot;foobar\u0026quot;, 10, pi), safely_divide2) safely_out ## [[1]] ## [[1]]$result ## [1] NA ## ## [[1]]$error ## \u0026lt;simpleError in .f(...): foobar is not a number!\u0026gt; ## ## ## [[2]] ## [[2]]$result ## [1] 5 ## ## [[2]]$error ## NULL ## ## ## [[3]] ## [[3]]$result ## [1] 1.570796 ## ## [[3]]$error ## NULL Following the approach suggested in the documentation of safely (in the examples section), we can use transpose() and simplify_all() to arrange the output.\nsimply_safe \u0026lt;- safely_out %\u0026gt;% transpose() %\u0026gt;% simplify_all() simply_safe$result # for the output values ## [1] NA 5.000000 1.570796 simply_safe$error # for the error messages ## [[1]] ## \u0026lt;simpleError in .f(...): foobar is not a number!\u0026gt; ## ## [[2]] ## NULL ## ## [[3]] ## NULL  For comparison‚Äôs sake, how would you tryCatch it? The function tryCatch is a base-R approach for error handling. The concept is similar but the syntax is different. Here is an example of how to build a safe divide2 function with tryCatch:\ntry_divide2 \u0026lt;- function(number){ tryCatch(expr = { divide2(number) }, error = function(e){ NA_real_ } ) } map(list(\u0026quot;foobar\u0026quot;, pi, 10), try_divide2) ## [[1]] ## [1] NA ## ## [[2]] ## [1] 1.570796 ## ## [[3]] ## [1] 5 As said - same output, different syntax. Choose your approach. As an appetizer, the same works with base-R functional programming as well. For example, with lapply it will look like this:\nlapply(list(\u0026quot;foobar\u0026quot;, pi, 10), try_divide2) ## [[1]] ## [1] NA ## ## [[2]] ## [1] 1.570796 ## ## [[3]] ## [1] 5  Bonus: a benchmark Before going off on your merry, error handled way, I also provide a short comparison between safely, possibly, and tryCatch.\nLet‚Äôs assume a data set with 20% errors. For simplicity, we‚Äôll use a modified log instead of our divide2 (similar to the example provided in purrr‚Äôs documentation). It makes sense to fail log in an all numeric vector (yield an error if there is a negative value). Since a negative log is NaN (not an error but rather a warning) I‚Äôm creating an error_prone_log function.\nlibrary(tibble) library(dplyr) library(microbenchmark) library(ggplot2) error_prone_log \u0026lt;- function(x){ if (x \u0026lt; 0){ stop(\u0026quot;Negative x\u0026quot;) } log(x) } safe_log \u0026lt;- safely(error_prone_log, otherwise = NaN, quiet = T) possible_log \u0026lt;- possibly(error_prone_log, otherwise = NaN, quiet = T) tryCatch_log \u0026lt;- function(x){ tryCatch(expr = error_prone_log(x), error = function(e){ NaN }) } set.seed(42) test_values \u0026lt;- sample( c(runif(n = 80, min = 0, max = 100), runif(n = 20, min = -100, max = 0)), size = 100, replace = FALSE) bench_results \u0026lt;- microbenchmark(map(test_values, safe_log), map(test_values, possible_log), map(test_values, tryCatch_log), times = 1000) gt::gt(summary(bench_results)) %\u0026gt;% gt::fmt_number(columns = 2:7, decimals = 2) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #acvreehjjd .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #acvreehjjd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #acvreehjjd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #acvreehjjd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; } #acvreehjjd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #acvreehjjd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #acvreehjjd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #acvreehjjd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #acvreehjjd .gt_column_spanner_outer:first-child { padding-left: 0; } #acvreehjjd .gt_column_spanner_outer:last-child { padding-right: 0; } #acvreehjjd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; } #acvreehjjd .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #acvreehjjd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #acvreehjjd .gt_from_md  :first-child { margin-top: 0; } #acvreehjjd .gt_from_md  :last-child { margin-bottom: 0; } #acvreehjjd .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #acvreehjjd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #acvreehjjd .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #acvreehjjd .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #acvreehjjd .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #acvreehjjd .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #acvreehjjd .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #acvreehjjd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #acvreehjjd .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #acvreehjjd .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #acvreehjjd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #acvreehjjd .gt_sourcenote { font-size: 90%; padding: 4px; } #acvreehjjd .gt_left { text-align: left; } #acvreehjjd .gt_center { text-align: center; } #acvreehjjd .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #acvreehjjd .gt_font_normal { font-weight: normal; } #acvreehjjd .gt_font_bold { font-weight: bold; } #acvreehjjd .gt_font_italic { font-style: italic; } #acvreehjjd .gt_super { font-size: 65%; } #acvreehjjd .gt_footnote_marks { font-style: italic; font-size: 65%; }   expr min lq mean median uq max neval cld    map(test_values, safe_log) 3.96 4.08 4.55 4.16 4.94 10.10 1000 c   map(test_values, possible_log) 3.74 3.86 4.27 3.92 4.64 9.60 1000 b    map(test_values, tryCatch_log) 2.97 3.05 3.45 3.09 3.68 73.99 1000 a     autoplot(bench_results) + ggtitle(\u0026quot;Comparison of safely, possibly, and tryCatch\u0026quot;) + coord_flip(ylim = c(1.5, 15)) As can be seen from the benchmark‚Äôs results, there is no doubt about it: tryCatch is the fastest. The purrr functions safely and possibly take longer to run. In my opinion though they are slightly more convenient in terms of syntax.\nIn addition, the safely variation allows us to retrieve the error messages conveniently for later examination. Again, this capability comes with a price when compared to tryCatch, but it is roughly the same run time when compared to possibly.\n Conclusion When you have a long looping process which is prone to errors, for example a pricey web API call or a really large data set, you should aim to catch and handle errors gracefully instead of hoping for the best.\nIf you really need to be efficient, it‚Äôs probably worth while to tryCatch, and if your looking more for ease of use and code readability you should use possibly. In case you need to examine the error messages more thoroughly, use safely.\nGood luck hunting down the errors!\n ","date":1605052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605083400,"objectID":"a5388ad1bf20c5fdb04f99756c2a9bf0","permalink":"https://adisarid.github.io/post/yet-another-post-on-error-handling/","publishdate":"2020-11-11T00:00:00Z","relpermalink":"/post/yet-another-post-on-error-handling/","section":"post","summary":"Error catching can be hard to catch at times (no pun intended). If you‚Äôre not used to error handling, this short post might help you do it elegantly.","tags":["purrr","Error handling"],"title":"YAPOEH! (Yet another post on error handling)","type":"post"},{"authors":null,"categories":["R"],"content":"  Corona has put us in an awkward situation, where we must rethink and revise our ways of doing things (teaching, working, baby sitting, balancing work and life, or any other related field of your choosing). I also see this as an opportunity to experiment and improve various things. Specifically I dedicate this post to some teaching methods I have adopted. Mostly: using breakout rooms (a zoom feature) and learnr tutorials.\nThe challenges I teach an undergraduate course in introduction to statistics and data analysis with R, in Tel-Aviv university. The course involves statistical theory and practice with R. In general students found challenging to follow both theory and practice, and more so, considering that all lectures are conducted via zoom, due to Corona constraints.\nIn order to alleviate some of these challenges, I decided to split the lectures to theory and labs in smaller groups.\n Methodology learnr is an R package which allows you to create an interactive html document (under the hood it‚Äôs based on RMarkdown and shiny). The result includes quizzes and chunks which can run R code. The chunks can come preloaded with some of the code and with hints/solutions.\nI teach a theoretical subject, and prepare a corresponding learnr tutorial on recent data sets from tidytuesday.\nDuring my lecture, I have specific checkpoints in which I split the zoom session into breakout rooms. Breakout rooms is a zoom feature which splits the main session into smaller sessions and allows the host to jump between the smaller sessions. I found that splitting the main room into groups of 3-4 participants which then together tackle the learnr tutorial is both fun and a fruitful experience. I cycle between the rooms to provide individual help for the groups. Every few minutes (10-15 minuts, depending on the tutorial section‚Äôs length and complexity), I merge the group back together to the main session, solve the tutorial section and either continue with theory or give them another section of the tutorial to work on in the smaller groups.\nEach tutorial includes important blocks of data analysis, sort of a ‚Äúmini project‚Äù such as: data import, visualizations, data transformations, and modeling. I direct the blocks according to the topic I covered in the theoretical part.\nHere are two examples for tutorials I made, which went very well in class:\n Food consumption footprint with the source code here. This one is about getting to know your data, visualizations, descriptive statistics, confidence intervals and hypothesis tests for the expectancy of a distribution. Volcanos with the source code here. This one is about independence Chi square test, and also includes a lot of data preparations and visualizations. By the end of it, students test if volcano type and elevation are related (independent statistically or not).   Important notes The first time you implement this, make sure the students understand how the zoom breakout feature works. Let them know you will be cycling through the rooms but that they can also use a ping/raise hand feature to call you.\nZoom comes with two options for assignment to breakout rooms: either random or manual (by host). Both are sub-optimal. Manually assigning 30 students to 10 breakout rooms will take too long and random ignores study groups the students already have. You can fine tune the random assignment, but that too takes too long. There is another option to upload a csv file with the participant‚Äôs names in advanced which is done via the settings of the meeting (on the zoom web page settings).\nWhen you setup the rooms, you have an option to set a time-limit. I found it sub-optimal, because you don‚Äôt always know in advanced how long the exercise will take. Instead cycling through the rooms should give you a good sense on when to merge back the breakout rooms into the main session.\nFor more help on using breakout rooms, follow this link from zoom‚Äôs support website.\n Conclusions This method, of using the zoom breakout rooms feature in combination with interactive learnr looks like a good formula for getting the students engaged while learning the necessary theoretical and practical aspects of data analysis.\nIf you are teaching interactively, I encourage you to give it a try! If you do, let me know how it went @SaridResearch on twitter.\n ","date":1590491160,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590491160,"objectID":"5779d9c22db36a8d8bf84d7ada307240","permalink":"https://adisarid.github.io/post/2020-05-26-break-rooms-and-learnr/","publishdate":"2020-05-26T11:06:00Z","relpermalink":"/post/2020-05-26-break-rooms-and-learnr/","section":"post","summary":"Corona has put us in an awkward situation, where we must rethink and revise our ways of doing things (teaching, working, baby sitting, balancing work and life, or any other related field of your choosing).","tags":["learnr"],"title":"Teaching with zoom breakout-rooms and the learnr package","type":"post"},{"authors":null,"categories":[],"content":"  For a while now I‚Äôve been struggling with various installation setup related to the open source versions of RStudio server, Shiny server (and dockerized versions of them).\nAfter browsing internet tutorials on-and-off for the last couple of weeks I‚Äôve come to the conclusion that there is a small gap when it comes to setting up RStudio server and shiny server securely, i.e., with SSL certificates.\nI‚Äôve put together this step-by-step post to close this gap (in part for self-documentation). I don‚Äôt cover dockerized versions here.\nStep 1 (or actually steps 1-10) Install rstudio server and shiny server on your choice of cloud provider. Common choices are AWS (EC2 or Lightsail, Digital Ocean, Azure, and Google. Anything works actually).\nInstallation instructions are covered in detail in Dean Attali‚Äôs post here which uses Digital Ocean.\nNo use repeating everything in its entirety. I will however highlight two things I‚Äôve found helpful and are a bit different from Dean‚Äôs post.\nSelection of instance size (resources) AWS Lightsail‚Äôs deafult firewall settings  Selection of instance size I‚Äôve found that opening an instance with low memory (i.e., 0.5-1Gb) is problematic. Its cheap, but if you‚Äôre installing packages such as dplyr (included in tidyverse), compiling the package within your server, will require more memory than what you have available. You can workaround this by increasing your swap file size (step 6 in Dean‚Äôs post), but eventually even R processes might give you a hard time in the future. I‚Äôve been using a 4GB server, which seems to be ok for my purposes (and at the time of writing this post costs about $20 USD per month).\n AWS Lightsail‚Äôs firewall settings When using AWS Lightsail (which I‚Äôve been using), a default firewall rule is applied. This means that when you complete steps 7-8 in Dean‚Äôs post, your server will not work if you‚Äôre using AWS Lightsail. The reason for that is that AWS Lightsail built-in firewall blocks all ports except 22 and 80 by default (rstudio server uses 8787 and shiny server uses 3838). It doesn‚Äôt matter much, because by the end of this post we‚Äôll be using port 443 (for secure SSL connection), but you can also make the non-secure version work out - useful for checking that your server is working. Let‚Äôs go ahead and open port 443 in your Lightsail instance (and explain how to also temporarily open 8787 and 3838).\nClick on the instance you‚Äôve opened, and then go to the Networking tab. In this page you‚Äôll see a ‚ÄúFirewall‚Äù title and beneath it a table with the currently open ports. Under the table click on ‚Äú+ Add another‚Äù. Add HTTPS (under application), TCP (under protocol) and port 443, like in the following screenshot:\nIf you want to temporarily enable direct access to port 8787 and 3838 (to check that your installation went well), you can do that as well (use ‚ÄúCustom‚Äù under application, TCP for protocol, and port 8787 (repeat for 3838 in an additional line).\n  Secure you server with SSL encryption Since you will probably be passing data back and forth between your remote server and local computer, you want to secure this data transfer with encryption. The best way to do this is using SSL certificates (it‚Äôs actually called TSL, the name SSL refers to a deprecated protocol, but everyone seems to still be using the SSL initials, see here).\nThere is a free service called Let‚Äôs Encrypt which provides SSL certificates, which is what we‚Äôre going to use here. I‚Äôm actually adopting the approach of this tutorial.\nInstall Let‚Äôs Encrypt and get certficates Install the following software on your linux server\nsudo apt instal letsencrypt Update your nginx configuration as preperation for obtaining the let‚Äôs encrypt certificate. This step is needed because when requesting a certificate from let‚Äôs encrypt, the let‚Äôs encrypt server will try to authenticate your server.\nUse\nsudo nano /etc/nginx/sites-enabled/default And add the following (replace r.example.com with your domain):\nserver { listen 80; listen [::]:80; root /var/www/r.example.com/html; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; server_name r.example.com; } Get your SSL certificates using the following line, just replace r.example.com with your subdomain.\nletsencrypt certonly -a webroot --webroot-path=/var/www/r.example.com/html/ -d r.example.com  Update your nginx settings again\nsudo nano /etc/nginx/sites-enabled/default To have the following setup (remember to replace r.example.com with your domain):\nmap $http_upgrade $connection_upgrade { default upgrade; \u0026#39;\u0026#39; close; } # listens on port 80 and redirects traffic to secure alternative server { listen 80 default_server; listen [::]:80 default_server; server_name r.example.com; return 301 https://r.example.com$request_uri; } server { # SSL configuration listen 443 ssl; ssl_certificate /etc/letsencrypt/live/r.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/r.example.com/privkey.pem; ssl_protocols TLSv1.2; ssl_ciphers EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256:EECDH+3DES:RSA+3DES:!MD5; ssl_prefer_server_ciphers On; ssl_session_cache shared:SSL:128m; add_header Strict-Transport-Security \u0026quot;max-age=31557600; includeSubDomains\u0026quot;; ssl_stapling on; ssl_stapling_verify on; root /var/www/r.example.com/html; server_name _; # Reroute traffic to shiny server (i.e., reverse proxy for port 3838) location /shiny/ { proxy_pass http://127.0.0.1:3838/; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; rewrite ^(/shiny/[^/]+)$ $1/ permanent; } # Reroute traffic to rstudio server (i.e., reverse proxy for port 8787) location /rstudio/ { proxy_pass http://127.0.0.1:8787/; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } }  Your server should be working now, but since Let‚Äôs Encrypt certificates only last 90 days, lets put an automatically renewal process in place.\nsudo nano /opt/renewCerts.sh Paste the following text:\n#!/bin/sh # This script renews all the Let\u0026#39;s Encrypt certificates with a validity \u0026lt; 30 days if ! letsencrypt renew \u0026gt; /var/log/letsencrypt/renew.log 2\u0026gt;\u0026amp;1 ; then echo Automated renewal failed: cat /var/log/letsencrypt/renew.log exit 1 fi nginx -t \u0026amp;\u0026amp; nginx -s reload Make sure the script is owned and executable by root:\nchown root.root /opt/renewCerts.sh chmod u+x /opt/renewCerts.sh Add it to cron for auto execution:\nsudo crontab -e Add\n@weekly /opt/renewCerts.sh   All should be set! Go ahead and browse to your domain (e.g., https://r.example.com/rstudio). Check that you‚Äôre able to login properly and that all pages are secure on https.\n ","date":1583532000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583532000,"objectID":"9d527107842058e21b347763aa147fe3","permalink":"https://adisarid.github.io/post/2020-03-06-setup_rstudio_server_with_ssl/","publishdate":"2020-03-06T22:00:00Z","relpermalink":"/post/2020-03-06-setup_rstudio_server_with_ssl/","section":"post","summary":"For a while now I‚Äôve been struggling with various installation setup related to the open source versions of RStudio server, Shiny server (and dockerized versions of them).\nAfter browsing internet tutorials on-and-off for the last couple of weeks I‚Äôve come to the conclusion that there is a small gap when it comes to setting up RStudio server and shiny server securely, i.","tags":["RStudio server","nginx","Let's Encrypt"],"title":"Set up RStudio server with Let's Encrypt SSL certificate","type":"post"},{"authors":null,"categories":[],"content":"  I‚Äôm just on my way back from this year‚Äôs rstudio::conf. Here‚Äôs an account of a few of the things which I found interesting, inspiring, or that might have some other impact in the future. I‚Äôm listing them by order of appearance in the conference.\nThe RMarkdown and Interactive Dashboards Workshop I participated as a TA in the RMarkdown and Interactive Dashboard workshop led by Carl Howe and Yihui Xie. The workshop dealt with creating RMarkdown documents. Specifically adding interactivity using the flexdashboard package to turn a simple docuement into an interactive html with just a few yaml configurations.\nSome attention was also given to combining different programming languages within documents, i.e.¬†python via the reticulate package, but also css and many more.\nA lesser known feature (at least for me) was the crosstalk package which makes it really easy for different widgets in an RMarkdown document talk to each other.\nYihui talked about some cool tips and tricks and features of rmarkdown (and surrounding packages). For example:\n The use of the xaringan::infinite_moon_reader() to continously render an Rmd upon save (automatically knits the document) Creating animation inside Rmds LaTeX tricks within Rmds Caching time consuming Rmd chunks Many more  Actually, these are just a few, and there were 23 distinct tips. Yihui‚Äôs presentation is available here. All the materials of this workshop, and in fact all the workshops, is available in this github repo.\nHere‚Äôs an example of an animation of a wave in an Rmd file. To replicate the example, use the following code in the chunk specifications: {r, animation.hook='gifski', fig.height=3, interval = 0.15}.\ndata_set \u0026lt;- crossing(frame = seq(0, 360, 30), x = seq(0, 2*pi, pi/10)) %\u0026gt;% mutate(y = sin(x - frame*pi/360)) %\u0026gt;% nest(data = c(x,y)) map(data_set$data, ~{ ggplot(., aes(x, y)) + geom_line() }) Cool, isn‚Äôt it?\nIf you‚Äôre into animations, there are other packages which can also accomplish this, such as the gganimate package.\n Deploying End-to-End Data Science with Shiny, Plumber, and Pins A nice talk by Alex Gold from RStudio. Showed a use case for a plumber api combined with a shiny app that talk to each other. For me, mainly the concept of pins as a quick approach for small to medium sized data sets was nice to see. You can easily put a data set in an S3 or any other means of storage on Azure, GCP, Github, etc. The downside compared to a database is that there is no frequent backup or an option to roll back changes.\n We‚Äôre hitting R a million times a day so we made a talk about it A talk by Heather and Jacqueline Nolis from T-Mobile. This was kind of a follow-up talk on what they discussed in the previous rstudio::conf (2019), but this time they came with some important lessons from their in-field experiences.\nFor example, talking about garbage collection in R which happens once in a while and causes response times to lag. Can be avoided by manually calling the garbage collection with gc().\n Asynchronous programming in R Winston Chang demonstrating some ways for Asynchronous programming with the later package.\nAn interesting thing he Winston demonstrated was that you can open up a server using websocket and httpuv to interact with the R process even if the rstudio console is busy (i.e., for example during a shiny app run).\nMaterials of the talk will probably be available here https://github.com/wch/2020-01-later, though as of the time of writing these lines, the repo is empty. Maybe later.\n Of Teacups, Giraffes, \u0026amp; R Markdown A lovely and ispiring talk by Desiree De Lean, about how she (and a co-author) used a sort of Gamification to develop a statistics course. The course is based on the mysterious world of teacup Giraffes, and introduces statistical concepts with a nice and appealing twist.\nAnother innovative approach this used was to combine learnr iframes within the RMarkdown book they used, which makes the experience of the learner interactive. The learnr segments are hosted in turn on shinyapps.io.\nThe book is available online here.\n Styling Shiny apps with Saas and Bootstrap 4 Joe Chang introduced the bootstraplibpackage which provides much more flexibilty when composing your own theme for a shiny app. It takes away a lot of the pains related with CSS-ing your way aound the complexities of a shiny app UI.\nThe package is still experimental, more info here.\nI talked to Joe Chang a bit, the package is not going to solve problems such as right-to-left localization for Hebrew.\n 3D ggplots with rayshader A cool presentation by Tyler Morgan Wall author of the rayshader package. Demonstrated how ggplot2 charts can be easily rendered into a 3d model.\nMostly relevant for illustrating surfaces. The rayshader can be used to turn this plot:\nsurface \u0026lt;- crossing(x = seq(0, 2*pi, pi/20), y = seq(0, 2*pi, pi/20)) %\u0026gt;% mutate(z = sin(x)*x + cos(3*y)) surface_plot \u0026lt;- ggplot(surface, aes(x = x, y = y, fill = z)) + geom_tile() surface_plot Projected as a 3d model using the rayshader here:\nrayshader::plot_gg(surface_plot) Tyler also demonstrated how you can generate a movie with the camera browsing around the chart. Beware, rendering takes long.\n Summary Obviously, there were many more notable talks at the conference, but this post was meant as a short list highlighting just a few.\nTo sum up, the rstudio conf, was a great opportunity to meet up with collegues (old and new), brush up on some of the advanced and noval packages, see how the rstudio team is planning and seeing the future of RStudio and of the R ecosystem, and enjoy San Fransisco!\n ","date":1580486880,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580486880,"objectID":"613765b194701be693f1401cb31b61b9","permalink":"https://adisarid.github.io/post/2020-01-31-rstudio-conf2020-recap/","publishdate":"2020-01-31T06:08:00-10:00","relpermalink":"/post/2020-01-31-rstudio-conf2020-recap/","section":"post","summary":"I‚Äôm just on my way back from this year‚Äôs rstudio::conf. Here‚Äôs an account of a few of the things which I found interesting, inspiring, or that might have some other impact in the future.","tags":["rstudio conference"],"title":"Impressions and notes from rstudio::conf2020","type":"post"},{"authors":null,"categories":["R"],"content":"  This semester I started teaching introduction to statistics and data analysis with R, at Tel-Aviv university.\nI put in a lot of efforts into bringing practical challenges, examples from real life, and a lot of demonstrations of statistical theory with R. This post is an example for how I‚Äôve been using R code (and specifically Shiny apps) to demonstrate statistical theory, concepts and provide intuition.\nWhat‚Äôs the difference between confidence and prediction intervals? Last week I taught multiple linear regression, and I noticed that students have a hard time comprehending the difference between confidence intervals and prediction intervals. The former being an interval for the model (i.e., interval for the underlying model), and the latter being an interval for a noval observation.\nAs the sample size increases, our uncertainty of the model‚Äôs parameters decreases, but the uncertainty in the value of a new observation, \\(y_0\\) is associated with variance of \\(Y\\) (the random variable from which \\(y_0\\) is drawn). Hence, it has a lower bound, based on that variance.\nIn R, you can get a prediction or a confidence interval by using either\npredict(object, newdata, interval = \"prediction\")\nOr\npredict(object, newdata, interval = \"confidence\")\nFor a prediction or for a confidence interval, respectively.\nTo help me illustrate the differences between the two, I decided to build a small Shiny web app. It shows the differences between confidence intervals, prediction intervals, the regression fit, and the actual (original) model.\nThe app is available here, and the source code is available on github.\nWith this app you can choose three types of models to demonstrate. Simple linear regression, and regression with a twist (\\(\\log\\) transformation on the \\(y\\) or \\(\\sin\\) transformation on the \\(x\\):\n Linear model \\(y = a + bx + \\epsilon\\)\n Log-linear model \\(\\log(y)=a+bx+\\epsilon\\)\n Sine \\(y = a + b\\sin(x) + \\epsilon\\)\n  All the models are based on simple linear regression (lm function), for the latter two models with either a log or sin transformation.\nThe app allows you to play around with various values such as the \\(x\\) range, the model‚Äôs parameters (\\(a\\) and \\(b\\)), the error‚Äôs standard deviation (\\(\\epsilon\\)), and show or hide any of the following elements, on the chart:\n The original function (i.e., the original model)\n The sampled points\n The confidence interval\n The prediction interval\n The model‚Äôs fit\n  Feel free to share the app or the app‚Äôs code. As mentioned above, the source code for the app is available here: https://github.com/adisarid/prediction_confidence_intervals_demo.\nHere‚Äôs an example for what the app‚Äôs generating code and output looks like, for a model of the type \\(\\log(y) = 1 + \\frac{x}{2} + \\epsilon\\):\nlibrary(dplyr) library(tidyr) library(tibble) library(ggplot2) sample_size \u0026lt;- 90 x_range \u0026lt;- c(0, 1.5) a \u0026lt;- 1 b \u0026lt;- 1.5 sigma \u0026lt;- 0.3 actual_function \u0026lt;- tibble(x = seq(x_range[1], x_range[2], by = 0.01)) %\u0026gt;% mutate(actual_y = exp(a + b*x)) random_sample \u0026lt;- tibble(epsilon_err = rnorm(n = sample_size, mean = 0, sd = sigma), x = runif(n = sample_size, min = x_range[1], max = x_range[2])) %\u0026gt;% mutate(sampled_y = exp(a + b*x + epsilon_err)) linear_model \u0026lt;- lm(formula = log(sampled_y) ~ x, data = random_sample) prediction_i \u0026lt;- predict(object = linear_model, newdata = actual_function, interval = \u0026quot;prediction\u0026quot;) %\u0026gt;% as_tibble() %\u0026gt;% rename_at(vars(lwr,upr), ~paste0(., \u0026quot;_pi\u0026quot;)) %\u0026gt;% mutate_all(exp) confidence_i \u0026lt;- predict(object = linear_model, newdata = actual_function, interval = \u0026quot;confidence\u0026quot;) %\u0026gt;% as_tibble() %\u0026gt;% rename_at(vars(lwr,upr), ~paste0(., \u0026quot;_ci\u0026quot;)) %\u0026gt;% select(-fit) %\u0026gt;% mutate_all(exp) intervals \u0026lt;- actual_function %\u0026gt;% bind_cols(prediction_i, confidence_i) ggplot() + geom_line(data = actual_function, aes(x, actual_y, color = \u0026quot;Original Model\u0026quot;), size = 1) + geom_point(data = random_sample, aes(x, sampled_y), alpha = 0.5) + geom_line(data = intervals, aes(x, fit, color = \u0026quot;Regression Fit\u0026quot;), size = 1) + geom_line(data = intervals, aes(x, lwr_pi, color = \u0026quot;Prediction Interval\u0026quot;), linetype = 2, size = 1) + geom_line(data = intervals, aes(x, upr_pi, color = \u0026quot;Prediction Interval\u0026quot;), linetype = 2, size = 1) + geom_line(data = intervals, aes(x, lwr_ci, color = \u0026quot;Confidence Interval\u0026quot;), linetype = 2, size = 1) + geom_line(data = intervals, aes(x, upr_ci, color = \u0026quot;Confidence Interval\u0026quot;), linetype = 2, size = 1) + theme_bw() + xlab(\u0026quot;x\u0026quot;) + ylab(\u0026quot;y\u0026quot;) + ggtitle(\u0026quot;Log-linear: Model, Fit, Confidence and Prediction Intervals\u0026quot;)  Conclusions Shiny apps are a great way to illustrate theoretical concepts, to provide intuition, and to let students experiment with parameters and see the outcomes. In this post I demonstrated how a Shiny app can be used to explain the concepts of a regression fit, confidence, and prediction intervals.\nIf you used Shiny for interesting educational demonstrations I‚Äôd love to hear about it! feel free to share in the comments or message me on twitter @SaridResearch.\n ","date":1576238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576238400,"objectID":"70d819906bc1c287dc95c1bf14a45fc9","permalink":"https://adisarid.github.io/post/2019-12-13-confidence_prediction_intervals_explained/","publishdate":"2019-12-13T12:00:00Z","relpermalink":"/post/2019-12-13-confidence_prediction_intervals_explained/","section":"post","summary":"This semester I started teaching introduction to statistics and data analysis with R, at Tel-Aviv university.\nI put in a lot of efforts into bringing practical challenges, examples from real life, and a lot of demonstrations of statistical theory with R.","tags":["linear models","Shiny"],"title":"Confidence and prediction intervals explained... (with a Shiny app!)","type":"post"},{"authors":null,"categories":["R"],"content":"  Google drive is a great tool, specifically we‚Äôve been using ‚ÄúG Suite‚Äù (the equivalent of google drive but for businesses), for a long time. Lately I noticed it‚Äôs missing an important feature - monitoring file shares and permission of google drive items across organization is non-trival (at least in the G suite basic subscription).\nI wanted to get a better sense of how my files and folders are shared across users within and outside the organization. I decided to give the googledrive package a try and extract the shares and permissions of important folders I had on my account. Here‚Äôs how I did that.\nUsing googledrive to mass extract shares and permission of google drive items First, I wanted to focus on specific folders which contain a lot of subfolders. My goal was to generate a tibble with the names of all sub-items (folder or files one level under the specific folders), along with all users which have access to these folders.\nHere is a description of what you need to do in order to accomplish what I described, followed by the code I used.\nGet the id (or URL) for the folder you want to retrieve data from. You can get the id by just visiting the folder, and the id is in the URL, i.e., https://drive.google.com/drive/u/1/folders/\u0026lt;HERE YOU WILL SEE THE FOLDER ID\u0026gt;. Then, retrieve all items within the folder. Use purrr functions (i.e., map and map_df to iterate over the results and bring them into a tidy form). (Optional) Use pivot_wider to create a tibble in a wide format where each row is an item and each column is the type of access each user has on the item.  The first function I‚Äôm using is a function I defined called get_permissions_df():\nlibrary(tidyverse) library(googledrive) # the package used to iterface with the google api # Function to retrieve email addresses of permissions (read/write) -------- get_permissions_df \u0026lt;- function(permission_list){ map_df(permission_list, ~{ if (!is.null(.$emailAddress)){ tibble(user_email = .$emailAddress, user_role = .$role) } else { tibble(user_email = NA, user_role = NA) } }) } The function returns a tibble with two columns: user_email and user_role, according to the users with access to the folder (access can be owner/writer/reader).\nNow, to actually pulling the data and processing it:\n# Read the contents of the folder ----- # note that the first time you run this, you will be asked to login into your gmail using a web browser. folder_contents \u0026lt;- drive_ls(as_id(\u0026quot;\u0026lt;FOLDER ID OR URL\u0026gt;\u0026quot;)) # replace here with the URL or ID of the folder. # Get a tidy form of all shares and permissions of subfolders tidy_permissions \u0026lt;- folder_contents %\u0026gt;% mutate(new_creds = map(drive_resource, ~{get_permissions_df(.$permissions)}) ) %\u0026gt;% select(name, new_creds) %\u0026gt;% unnest(new_creds) %\u0026gt;% filter(!is.na(user_email)) # Optional - turn into a wider form where each column is a user, # each row is a subfolder, and values are permissions of users. wide_permissions \u0026lt;- tidy_permissions %\u0026gt;% distinct(name, user_email, .keep_all = T) %\u0026gt;% pivot_wider(id_cols = name, names_from = user_email, values_from = user_role, values_fill = list(user_role = \u0026quot;none\u0026quot;)) There you have it: tidy_permissions will hold the names of all subfolders with permissions. A folder will appear as many times as it has permissions (with the user email and the type of permission). The wide_permissions will hold a wide version in which each row is a folder and each column is a user.\nNote that this works for specific folders. You can also use drive_ls() without any arguments (or use it with recursive=TRUE), to pull everything on the drive (or everything within all subfolders, recursively). When I did that it took me around 5-10 minutes to pull all the data and about 5 minutes to prepare it, since I have \\(\u0026gt;100k\\) items.\n Conclusions The post provides a method to create a concise tibble with the contents of you google drive items, and their user permissions. You can either run it on all items in your google drive or on selected folders (and sub-folders within). The method is especially useful in the context of data safety and security, when you want to make sure you are not sharing sensitive items in an undesired manner.\n ","date":1568721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568721600,"objectID":"35626b96f8c7e9b9481619de13716345","permalink":"https://adisarid.github.io/post/2019-09-17-google_drive_dir_structure_permissions/","publishdate":"2019-09-17T12:00:00Z","relpermalink":"/post/2019-09-17-google_drive_dir_structure_permissions/","section":"post","summary":"Google drive is a great tool, specifically we‚Äôve been using ‚ÄúG Suite‚Äù (the equivalent of google drive but for businesses), for a long time. Lately I noticed it‚Äôs missing an important feature - monitoring file shares and permission of google drive items across organization is non-trival (at least in the G suite basic subscription).","tags":["googledrive"],"title":"Retrieving google drive item shares and permissions (in R)","type":"post"},{"authors":null,"categories":["R"],"content":"  Background Shiny apps are a great way to share information and empower your users. Sometimes you want to make sure that only authenticated and authorized users will be able to view your shiny apps.\nThere are a number of ways to make sure only certain users have access to your apps. For example, you can subscribe to the professional plan in shinyapps.io which has this option built-in. You can program the authentication flow internally by yourself, or you just use a 3rd party service such as google firebase, AWS Cognito, Auth0, or others).\nThe benefit of using a dedicated service is that you get a lot of features which will be a serious headake to program yourself, such as social logins, two factor authentication, logs, and user blocks on suspicious attempts (or warnings on unauthorized attempts, depending on settings).\nThe down side is that it takes some time to implement. In this guide I aim to make the process as simple and painless as possible, using the Amazon Web Service‚Äôs authentication solution, called AWS Cognito. But first, some theory about authentication.\n How authentication works The logic behind authentication with AWS Cognito (or similar alternatives) is that you direct your users to a login page hosted by AWS, in which the user completes a process which confirms the user‚Äôs indentity. For example, by entering an e-mail and password, or by using a social sign-in (i.e., login via gmail, amazon, facebook). Then, once Cognito is finished, the user is redirected to your app with a URL variable which contain a specially issued code (i.e., https://your-app-address/?code=AMAZON_ISSUED_CODE.\nThen, you use an http request (i.e., with package httr) to query the Cognito API with this code, and in return you receive the information behind this code (i.e., the user‚Äôs token, with information such as the name of the user, what is the user‚Äôs email, validity of the token, etc.). This httr query is performed by using a password known only to you (i.e., only your app ‚Äúknows‚Äù this password, this is not the user‚Äôs password).\nThe code is usable only once, and the token is valid for a limited duration, to minimize the risk that an unauthorized party will hijack the token and re-use it to access your app.\nAfter authenticating the user, you can authorize the user according to privileges (which you would have to manage within your app, i.e.¬†if the users email is X, then he can view Y).\nThis process description was a very simplified, down-to-earth, nutshell description of oauth2. It might be inaccurate, but it will be enough for our goal here which is to actually implement it within a shiny app, integrating to Cognito. If you wish, you can find more information about oauth2 in detail here.\nLet‚Äôs get to business.\n Step 1: Define a user pool This step is actually performed within the AWS Console. Log into your AWS console and find the Cognito service. Click on ‚ÄúManage User Pools‚Äù, and then create a new user pool. The step-by-step wizard is pretty self explanatory, so I‚Äôll focus on the important things:\n Make sure that you require a relevant field upon your user sign-up which you can ‚Äúcount on‚Äù in order to perform user authorization within your app based on that field later on. I usually check the email address as a required field, and then add logic in my app which maps email addresses to what each user is allowed to view. Multi-factor authentication can be ‚Äúoff‚Äù, ‚Äúoptional‚Äù or ‚Äúrequired‚Äù. If your app contains sensitive information, then you should consider making it required.  Important! in the step where you are asked ‚ÄúWhich app clients will have access to this user pool?‚Äù click on ‚ÄúAdd an app client‚Äù. Give your app a name, the deafult options are sufficient so you shouldn‚Äôt change anything.\nMake sure you click on ‚ÄúShow Details‚Äù after you added your app and document the App client id and the App client secret. You will need them later on to interact with the Cognito API.\nOptional: right after you add your app and click ‚Äúnext step‚Äù, you will have a chance to add functions triggered by the various steps of the authentication flow. If you know what AWS Lambda functions are (and you defined such functions in your account) you can choose to trigger them depending on the authentication flow.\nComplete the wizard and create your user pool.\nEmail communications You must have the AWS SES (simple email service) configured properly, in order for the registration of new users and ‚Äúforgot password‚Äù flows to work. By default, SES is in sandbox mode, which means you can only register users with pre-verified emails. Defining SES is outside the scope of this guide, but note that you have to open a ticket in the AWS support center, asking for these privilleges.\nMake sure you supply AWS support with a lot of information about how you make sure emails don‚Äôt bounce, and about spam prevention. Even though it‚Äôs trivial, since this is an internal AWS system using the email service, they made me jump through hoops, untill granting me a 50k daily email cap, which is more than enough for me.\n App client settings Under the ‚ÄúApp integration -\u0026gt; App client settings‚Äù you need to add the Callback URL of your app (where the user is directed upon login). For example, if your app is going to be hosted on shinyapps.io that would be: https://YOUR_USER_NAME.shinyapps.io/YOUR_APP_NAME. Your sign-out url can be the same, if you want the app to allow the user to restart the login, or a different page showing that the user has logged out.\nUnder OAuth 2.0/‚ÄúAllowed OAuth Flows‚Äù you should check the: Authorization code grant. This is the authentication flow we are going to use for our shiny app. The ‚Äúimplicit grant‚Äù is not as secure, and the ‚Äúclient credentials‚Äù is used for machine-to-machine authentication.\nUnder ‚ÄúAllowed OAuth Scopes‚Äù check the options by which you are going to recognize your users within the shiny app‚Äôs logic. I.e., if you are going to show specific data by the user‚Äôs email address than make sure you check the ‚Äúemail‚Äù under allowed OAuth scopes.\nSet a domain name for your login screen and customize the UI of the login screen if you wish.\nYou can see in the following screenshot, that I‚Äôm using this authentication with one of my apps hosted in a shinyapps.io domain, under my account.\n Enable identity provides (Optional) If you want to offer your users a social login such as Facebook or Google, you would need to issue the proper credentials via google console and facebook. This is a nice addition, but is out of the scope of this guide.\nFinally, we get into the R code part of this post.\n  Step 2: Authorization code (within R) Now we need to add logic to our shiny app which will redirect the user to the AWS Cognito login page, and once the user authenticates and redirected to the shiny app, our shiny app will verify the token‚Äôs validity.\nVery basically, the Shiny app should read query url variables, and:\nIf no variables appear, show a login button to the user (which will redirect to the AWS Cognito login screen with the proper parameters). If a url variable called code appears, our app will read its value, and use AWS Cognito to apply a second layer of verification and identification according to the code (read the token issued by Cognito). If the user is logged on, show a ‚Äúlogout‚Äù button which will redirect the user into AWS Cognito logout link.  Redirect links to login/logout screen (AUTHORIZATION, LOGOUT Endpoints) There are two ‚Äúendpoints‚Äù (urls) that your users will be redirected to:\nDuring their login flow into the app, or; After they click ‚Äúlogout‚Äù.  The first is the ‚ÄúAUTHORIZATION Endpoint‚Äù. It is a redirection of the client to a url of the following form (I have already put it into a paste0 command, which we will later use in our app).\nbase_cognito_url \u0026lt;- \u0026quot;https://YOUR_PREEDEFINED_AWS_COGNITO_DOMAIN.amazoncognito.com/\u0026quot; app_client_id \u0026lt;- \u0026quot;YOUR_APP_CLIENT_ID\u0026quot; app_client_secret \u0026lt;- \u0026quot;YOUR_APP_CLIENT_SECRET\u0026quot; redirect_uri \u0026lt;- \u0026quot;https://YOUR_APP/redirect_uri\u0026quot; # e.g., if you are using shinyapps.io this would be: # https://ACCOUNT_NAME.shinyapps.io/YOUR_APP_NAME aws_auth_redirect \u0026lt;- paste0( base_cognito_url, \u0026quot;oauth2/authorize?\u0026quot;, \u0026quot;response_type=code\u0026amp;\u0026quot;, \u0026quot;client_id=\u0026quot;, app_client_id, \u0026quot;\u0026amp;\u0026quot;, \u0026quot;redirect_uri=\u0026quot;, redirect_uri, \u0026quot;\u0026amp;\u0026quot;, \u0026quot;state=appredirect\u0026quot; ) aws_auth_redirect ## [1] \u0026quot;https://YOUR_PREEDEFINED_AWS_COGNITO_DOMAIN.amazoncognito.com/oauth2/authorize?response_type=code\u0026amp;client_id=YOUR_APP_CLIENT_ID\u0026amp;redirect_uri=https://YOUR_APP/redirect_uri\u0026amp;state=appredirect\u0026quot; You can also specify the ‚Äúscopes‚Äù (what information should Cognito hold for your next query, email, phone, etc.). If you don‚Äôt specify any scopes, all the information available on the user will be provided (see the next section of this post ‚ÄúQuerying Cognito with the grant code‚Äù).\nThe second endpoint is the ‚ÄúLOGOUT Endpoint‚Äù which will logout the user. It is important to provide a logout button so that users can safely close your app, without worrying about other users in the same computer abusing their credentials.\naws_auth_logout \u0026lt;- paste0( base_cognito_url, \u0026quot;logout?\u0026quot;, \u0026quot;client_id=\u0026quot;, app_client_id, \u0026quot;\u0026amp;\u0026quot;, \u0026quot;logout_uri=\u0026quot;, redirect_uri ) aws_auth_logout ## [1] \u0026quot;https://YOUR_PREEDEFINED_AWS_COGNITO_DOMAIN.amazoncognito.com/logout?client_id=YOUR_APP_CLIENT_ID\u0026amp;logout_uri=https://YOUR_APP/redirect_uri\u0026quot; Once a user has completed the login process (via the authorization endpoint), he will be redirected to your app (the link you provided in the redirect_uri and in the Cognito setup at step 1). If the login is successful, the user will return with a url variable called code, i.e.¬†https://YOUR_APP/redirect_uri?code=####-####-####-####. The next step will be to make sure that this code is indeed valid, and to check who is the user trying to access behind this code.\n Querying Cognito with the grant code This is a crucial part, in which we make sure that the user is indeed valid, and allowed to access your app. We‚Äôre going to use the httr package for that.\nLet‚Äôs assume we have already pulled the authorization code from the Shiny app‚Äôs url variables (we‚Äôre going to show how to do that in step 3).\nWe‚Äôre going to build a function which gets the code as an argument and provides the user‚Äôs information (or an error if the user is not authenticated or there was a different failure). I usually place this code in my global.r file, which is a part of the shiny app‚Äôs bundle (ui.r, server.r, global.r), and is used to define an environment variables and functions which will be availble to the shiny app. You can also place it at the begining of the server.r if you don‚Äôt want a global.r file. If you are using a single app.r just put it before the app itself.\nHere is the code that goes into your global.r file:\nbase_cognito_url \u0026lt;- \u0026quot;https://YOUR_DOMAIN.YOUR_AMAZON_REGION.amazoncognito.com/\u0026quot; app_client_id \u0026lt;- \u0026quot;YOUR_APP_CLIENT_ID\u0026quot; app_client_secret \u0026lt;- \u0026quot;YOUR_APP_CLIENT_SECRET\u0026quot; redirect_uri \u0026lt;- \u0026quot;https://YOUR_APP/redirect_uri\u0026quot; library(httr) app \u0026lt;- oauth_app(appname = \u0026quot;my_shiny_app\u0026quot;, key = app_client_id, secret = app_client_secret, redirect_uri = redirect_uri) cognito \u0026lt;- oauth_endpoint(authorize = \u0026quot;authorize\u0026quot;, access = \u0026quot;token\u0026quot;, base_url = paste0(base_cognito_url, \u0026quot;oauth2\u0026quot;)) retrieve_user_data \u0026lt;- function(user_code){ failed_token \u0026lt;- FALSE # get the token tryCatch({token_res \u0026lt;- oauth2.0_access_token(endpoint = cognito, app = app, code = user_code, user_params = list(client_id = app_client_id, grant_type = \u0026quot;authorization_code\u0026quot;), use_basic_auth = TRUE)}, error = function(e){failed_token \u0026lt;\u0026lt;- TRUE}) # check result status, make sure token is valid and that the process did not fail if (failed_token) { return(NULL) } # The token did not fail, go ahead and use the token to retrieve user information user_information \u0026lt;- GET(url = paste0(base_cognito_url, \u0026quot;oauth2/userInfo\u0026quot;), add_headers(Authorization = paste(\u0026quot;Bearer\u0026quot;, token_res$access_token))) return(content(user_information)) }   Step 3: define your Shiny app‚Äôs server.r and ui.r In our shiny app, we need to pull the code and use the retrieve_user_data function we‚Äôve just defined as part of our verification of the user. Here is the code we will use for this. This should go into the server.r file.\nlibrary(shiny) library(shinyjs) # define a tibble of allwed users (this can also be read from a local file or from a database) allowed_users \u0026lt;- tibble( user_email = c(\u0026quot;user1@example.com\u0026quot;, \u0026quot;user2@example.com\u0026quot;)) function(input, output, session){ # initialize authenticated reactive values ---- # In addition to these three (auth, name, email) # you can add additional reactive values here, if you want them to be based on the user which logged on, e.g. privileges. user \u0026lt;- reactiveValues(auth = FALSE, # is the user authenticated or not name = NULL, # user\u0026#39;s name as stored and returned by cognito email = NULL) # user\u0026#39;s email as stored and returned by cognito # get the url variables ---- observe({ query \u0026lt;- parseQueryString(session$clientData$url_search) if (!(\u0026quot;code\u0026quot; %in% names(query))){ # no code in the url variables means the user hasn\u0026#39;t logged in yet showElement(\u0026quot;login\u0026quot;) } else { current_user \u0026lt;- retrieve_user_data(query$code) # if an error occurred during login if (is.null(current_user)){ hideElement(\u0026quot;login\u0026quot;) showElement(\u0026quot;login_error_aws_flow\u0026quot;) showElement(\u0026quot;submit_sign_out_div\u0026quot;) user$auth \u0026lt;- FALSE } else { # check if user is in allowed user list # for more robustness, use stringr::str_to_lower to avoid case sensitivity # i.e., (str_to_lower(current_user$email) %in% str_to_lower(allowed_users$user_email)) if (current_user$email %in% allowed_users$user_email){ hideElement(\u0026quot;login\u0026quot;) showElement(\u0026quot;login_confirmed\u0026quot;) showElement(\u0026quot;submit_sign_out_div\u0026quot;) user$auth \u0026lt;- TRUE user$email \u0026lt;- current_user$email user$name \u0026lt;- current_user$name # ==== User is valid, continue prep ==== # show the welcome box with user name output$confirmed_login_name \u0026lt;- renderText({ paste0(\u0026quot;Hi there!, \u0026quot;, user$name) }) # ==== Put additional login dependent steps here (e.g. db read from source) ==== # ADD HERE YOUR REQUIRED LOGIC # I personally like to select the first tab for the user to see, i.e.: showTab(\u0026quot;main_navigation\u0026quot;, \u0026quot;content_tab_id\u0026quot;, select = TRUE) # (see the next chunk for how this tab is defined in terms of ui elements) # ==== Finish loading and go to tab ==== } else { # user not allowed. Only show sign-out, perhaps also show a login error message. hideElement(\u0026quot;login\u0026quot;) showElement(\u0026quot;login_error_user\u0026quot;) showElement(\u0026quot;submit_sign_out_div\u0026quot;) } } } }) # This is where you will put your actual elements (the server side that is) ---- # For example: output$some_plot \u0026lt;- renderPlot({ # *** THIS IS EXTREMELY IMPORTANT!!! *** validate(need(user$auth, \u0026quot;No privileges to watch data. Please contact support.\u0026quot;)) # since shinyjs is not safe for hiding content, make sure that any information is covered # by the validate(...) expression as was specified. # Rendered elements which were not preceded by a validate expression can be viewed in the html code (even if you use hideElement). # only if user is confirmed the information will render (a plot in this case) plot(cars) }) }  The accompanying user interface (ui.r) will look like the following:\n library(shiny) library(shinyjs) fluidPage( useShinyjs(), # to enable the show/hide of elements such as login and buttons hidden( # this is how the logout button will like: div( id = \u0026quot;submit_sign_out_div\u0026quot;, a(id = \u0026quot;submit_sign_out\u0026quot;, \u0026quot;logout\u0026quot;, href = aws_auth_logout, style = \u0026quot;color: black; -webkit-appearance: button; -moz-appearance: button; appearance: button; text-decoration: none; background:#ff9999; position: absolute; top: 0px; left: 20px; z-index: 10000; padding: 5px 10px 5px 10px;\u0026quot; ) ) ), navbarPage( \u0026quot;Cognito auth example\u0026quot;, id = \u0026quot;main_navigation\u0026quot;, tabPanel( \u0026quot;identification\u0026quot;, value = \u0026quot;login_tab_id\u0026quot;, h1(\u0026quot;Login\u0026quot;), div( id = \u0026quot;login\u0026quot;, p(\u0026quot;To login you must identify with a username and password\u0026quot;), # This defines a login button which upon click will redirect to the AWS Cognito login page a(id = \u0026quot;login_link\u0026quot;, \u0026quot;Click here to login\u0026quot;, href = aws_auth_redirect, style = \u0026quot;color: black; -webkit-appearance: button; -moz-appearance: button; appearance: button; text-decoration: none; background:#95c5ff; padding: 5px 10px 5px 10px;\u0026quot;) ), hidden(div( id = \u0026quot;login_error_aws_flow\u0026quot;, p(\u0026quot;An error has occurred.\u0026quot;), p(\u0026quot;Please contact support\u0026quot;) )), hidden( div( id = \u0026quot;login_confirmed\u0026quot;, h3(\u0026quot;User confirmed\u0026quot;), fluidRow( textOutput(\u0026quot;confirmed_login_name\u0026quot;)), fluidRow( p(\u0026quot;Use the menu bar to navigate.\u0026quot;), p( \u0026quot;Don\u0026#39;t forget to logout when you want to close the system.\u0026quot; ) ) ) ), ), tabPanel(\u0026quot;Your actual content\u0026quot;, value = \u0026quot;content_tab_id\u0026quot;, fluidRow(plotOutput(\u0026quot;some_plot\u0026quot;))) ) )   Conclusions The post contains essential things you need in order to get started with AWS Cognito authentication for your shiny apps.\nYou can extend this process to any authentication service (for example, digital ocean has a similar service to Cognito). There are some packages which implement the entire process for other services, like googleAuthR for a gmail login see this link.\nIf you found this post useful, let me know!, either in comments below, or twitter, or email.\nAs always, be careful of how you implement this process in your own apps, to make sure there are no security risks or loopholes. Also, DISCLAIMER: The information in this post is free, you can use this however like. Note that it is published with the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n ","date":1567166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567166400,"objectID":"1e909287f0b95d10802216934b789d64","permalink":"https://adisarid.github.io/post/2019-08-10-cognito-shiny-authentication/","publishdate":"2019-08-30T12:00:00Z","relpermalink":"/post/2019-08-10-cognito-shiny-authentication/","section":"post","summary":"Background Shiny apps are a great way to share information and empower your users. Sometimes you want to make sure that only authenticated and authorized users will be able to view your shiny apps.","tags":["Shiny","AWS","Cognito","Authentication"],"title":"Securing Shiny apps with AWS Cognito authentication","type":"post"},{"authors":null,"categories":["R"],"content":"  I‚Äôve been building R shiny apps for a while now, and ever since I started working with shiny, it has significantly increased the set of services I offer my clients.\nHere‚Äôs a documentations of some of the many lessons I learned in previous projects I did. Hopefully, others can avoid them in the future.\nBackground Shiny is a really great tool that allows data scientists to communicate their analysis in an appealing and an effective way. However, as data scientists we are used to thinking about things a certain way. Our way of thinking, and our practices are different than these of a software developer or a DevOps.\nHere are some of things I learned along the path of my Shiny app developing experiences - some things that you should and should not do.\n Don‚Äôt skip the planning phase Do a mockup, research your implementation options.\nMockup Do a mockup, even if it‚Äôs just a piece of paper which took you 5 minutes to draw. It will be worth it.\nShiny is very tempting in the sense that once you understand the concept of reactive programming, you can go from idea to a full app in a few days work. Why invest time in preparing a mockup or planning, when you can just go ahead and do the actual thing?\nMy experience tells me that the app is much more successful in capturing the customer‚Äôs needs, when he‚Äôs a part of the technical planning phase (when you share your dillemas with the client). It sets expectations, frames what you can and can‚Äôt (or won‚Äôt) do for the customer, and enables you to find solutions together.\nAlso, when you‚Äôre looking at a mockup (even if it‚Äôs just a simple drawing or a non-interactive slide), it helps in the next stages of building the app‚Äôs UI.\nHere is an example of how a mockup would look like when I‚Äôm drawing it on a piece of paper. Note how I‚Äôve already written down the purpose of some of the elements and their expeted element ids. It helps building the UI when you‚Äôre actually looking at one of these.\nExample for a mockup drawing\n  Research When you encounter a requirement you did not encounter before, and wondering about how to accomplish it, research.\n Is there more than a single way to accomplish what you‚Äôre trying? What are the pros and cons of each method?  For example, when I needed to show a table and incorporate data intake into the table, I was researching two options, one with the DataTable package (via the editable=TRUE argument) and the other is the rhandsontable package.\nBoth provide data editing, eventually I chose randsontable which had some limitations (e.g., slower rendering than DataTable, no search box), but provided more features out-of-the-box (e.g., data validation displaying factors as a list, checkboxes, etc.).\n  Be sure you can live up to your promises This is more of a broad issue (you can say its true for anything).\nIn my case, in the past I promised some clients I‚Äôll provide ‚Äúrealtime‚Äù dashboards. However, as it turned out, I was reading from a csv data dump which provided the data with delays going up to 15-30 minutes.\nIn most projects I do, 15 minutes and realtime are pretty much equivalent from a practical standpoint, but in a specific project I did recently, I had a client which wanted to check the data as it was changing minute-by-minute.\nThis gap in expectations caused some confusion and dissappointment. We eventually learned from this, and in the future, when realtime is a requirement, we will use a better data source (i.e., data base instead of the delayed data dump).\n Don‚Äôt forget to plan your budget Make sure you consider all the elements you need for the project. Plan the budget accordingly, and understand the ramifications of scaling the app.\nFor example, if you‚Äôre using shinyapps.io, get familiar with the pricing packages, figure out what will you need to provide a good SLA (relative to the number of users of the app).\nSame goes for other cloud services, e.g., using a data base - how many users? how many connections? size of data base?\nIn most cloud providers you can also set up billing alerts which lets you know when something is exceeding a predetermined threshold.\nAll of these are very important when you‚Äôre building your quote, and obviously when going into production with your App.\n Don‚Äôt skip testing and staging on your way to production In software development there are various levels of environments, starting from your desktop (‚Äúlocal‚Äù), through development server, integration, testing, staging/acceptance, and production. See Wikipedia‚Äôs Deployment environment entry.\nWhen building an app, make sure you go through these steps. Specifically relating to testing, staging, and production). What I found to be particularly useful is to upload the app twice (in two seperate locations/urls):\nDeploy as a beta app (client acceptance/demo) in which I demonstrate additional features and discuss them with the client, before incorporating them into production. Deploy as a production/live app.  As you iterate and improve the app, fix bugs, and add new features, you are also at the risk of breaking things. Thus, you should first update the beta app, share the new additions, and let the client experiment with the app. This way you can double check you didn‚Äôt break anything else.\nOnly when the client authorizes the corrections, redeploy the new app to the production.\n Conclusions As data scientists using Shiny, we‚Äôve also become software developers. We‚Äôre developing not just for ourselves or for other useRs in our community.\nWith Shiny we‚Äôre building for end-users. We‚Äôre building customer facing apps, and we need to keep that in mind. We should make sure that we adopt and use best practices of software development.\n ","date":1563537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563537600,"objectID":"8d10b47a62962dcb8ce6ad96e0cbfbaa","permalink":"https://adisarid.github.io/post/2019-07-03-shiny_app_lessons/","publishdate":"2019-07-19T12:00:00Z","relpermalink":"/post/2019-07-03-shiny_app_lessons/","section":"post","summary":"I‚Äôve been building R shiny apps for a while now, and ever since I started working with shiny, it has significantly increased the set of services I offer my clients.","tags":["Shiny"],"title":"What NOT to do when building a shiny app (lessons learned the hard way)","type":"post"},{"authors":null,"categories":["R"],"content":"  Over the last month I gave a tidyverse + intro to data science corporate training in a startup in Tel-Aviv. We had two groups (beginners and intermediates), and for the last assignment of the course I was aiming for a short quiz comprised of various topics which we covered during the course, such that can also be automated easily (i.e., multiple choice questions).\nI came up with the following quiz, which I thought would be nice to share here. I guess that experts should probably be able to complete this in a few minutes, intermediate/beginners would probably complete this by up to 30 minutes.\nExam instructions The following exam contains 10 questions which spans across the different topics regaring tidyverse, and some analysis dilemmas. Each question has four options but only one correct answer. Each correct answer provides you with 10 points.\nYou can use any materials you want including but not limited to: cheat sheets, our course materials, stack overflow, package documentation, running code and seeing what it does.\n Question 1: When would you use an R markdown file (.Rmd) versus a script file (.R) to save your work?\nIf I want the relative position of the file retained (so that it is easier to load files from the same directory), I will use an .Rmd file, otherwise I will use a .R file. When I want a complete documentation of my work in a report I will use a .Rmd. I will use a .R file for debugging and sourcing functions. There is no significant difference between the two formats, and they can be used for the same things interchangably. There is no benefit to using .R script files, the .Rmd format is always superior.   Question 2: Look at the following segments of code.\n# segment 1: new_data \u0026lt;- read.csv(\u0026quot;myfilename.csv\u0026quot;) # segment 2: new_data %\u0026gt;% group_by(some_cool_suff) %\u0026gt;% summarize(average = mean(avg_me, na.rm = T)) -\u0026gt; updated_df # segment 3: avg_var \u0026lt;- mean(new_data$avg_me[!is.na(some_cool_stuff)], na.rm = T) # segment 4: data.frame(a = 1:10, b = letters[1:10]) %\u0026gt;% sample_n(3)  Which segments would you classify as tidyverse syntax? (tidyverse syntax = code which uses functions from tidyverse packages, in which there is no function that you can replace to a tidyverse equivalent)\nSegment 1 and segment 3. Segment 2 and segment 4. Segment 4. Segment 2.   Question 3: What ggplot2 geoms would you use to generate the following charts?\nFigure 1: not generated with ggplot2, Figure 2: geom_point. Figure 1: geom_boxplot, Figure 2: geom_line. Figure 1: geom_violin, Figure 2: geom_point. Figure 1: geom_boxplot, Figure 2: geom_point + geom_line.  ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;  Question 4: What is the difference between the matrix and the tibble in the following?\nmatrix(cbind(1:10, letters[1:10], LETTERS[1:10]), ncol = 3) ## [,1] [,2] [,3] ## [1,] \u0026quot;1\u0026quot; \u0026quot;a\u0026quot; \u0026quot;A\u0026quot; ## [2,] \u0026quot;2\u0026quot; \u0026quot;b\u0026quot; \u0026quot;B\u0026quot; ## [3,] \u0026quot;3\u0026quot; \u0026quot;c\u0026quot; \u0026quot;C\u0026quot; ## [4,] \u0026quot;4\u0026quot; \u0026quot;d\u0026quot; \u0026quot;D\u0026quot; ## [5,] \u0026quot;5\u0026quot; \u0026quot;e\u0026quot; \u0026quot;E\u0026quot; ## [6,] \u0026quot;6\u0026quot; \u0026quot;f\u0026quot; \u0026quot;F\u0026quot; ## [7,] \u0026quot;7\u0026quot; \u0026quot;g\u0026quot; \u0026quot;G\u0026quot; ## [8,] \u0026quot;8\u0026quot; \u0026quot;h\u0026quot; \u0026quot;H\u0026quot; ## [9,] \u0026quot;9\u0026quot; \u0026quot;i\u0026quot; \u0026quot;I\u0026quot; ## [10,] \u0026quot;10\u0026quot; \u0026quot;j\u0026quot; \u0026quot;J\u0026quot; tibble(num = 1:10, sl = letters[1:10], cl = LETTERS[1:10]) ## # A tibble: 10 x 3 ## num sl cl ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 a A ## 2 2 b B ## 3 3 c C ## 4 4 d D ## 5 5 e E ## 6 6 f F ## 7 7 g G ## 8 8 h H ## 9 9 i I ## 10 10 j J The tibble has named variables (columns) and the matrix does not name the columns. The tibble retains the original data type and the matrix converts the data types. matrix is a base R function and tibble is a tidyverse function. All of the above.   Question 5: What stringr function would you use to simplify the following code?\nsome_string \u0026lt;- c(\u0026quot;How are you today?\u0026quot;, \u0026quot;Is this test ok?\u0026quot;, \u0026quot;You\u0026#39;re already half way in!\u0026quot;) map_chr(some_string, ~paste0(stringi::stri_wrap(., width = 5), collapse = \u0026quot;\\n\u0026quot;)) str_count. str_wrap. str_sub. No such function: must use a combination of a stringr and a loop (or a map function).   Question 6: What is the difference between contains and one_of?\nBoth are ‚Äúselect helpers‚Äù, one_of is used to specify strings which starts with one of the specified expressions, and contains lets you specify the variable names in ‚Äúnon standard evaluation‚Äù (unquoted) style. contains selects variables based on the regular expression you feed as an argument. one_of needs you to specify the variable names as strings. contains selects variables which contain the literal string you feed into it. one_of needs you to specify the variables names as strings. Both functions do the same thing with the same arguments.   Question 7: When reshaping data with the gather function, what is the meaning of the ... argument?\nSpecify which variables to gather by. Specify which variables not to gather by (using the ‚Äú-‚Äù sign). Specify either a or b. Provide variable by which to group the resulting tibble.   Question 8: What function would you use to get all the rows in tibble1 which are not in tibble2?\nsetdiff(tibble1, tibble2) setdiff(tibble2, tibble1) intersect(tibble1, tibble2) semi_join(tibble1, tibble2)   Question 9: Assume you examine the data which appears in the following scatter plot using per-axis boxplots. Would classify point A as an outlier?\nYes, only accoring to the y-axis. Yes, only according to the x-axis. Yes, according to either x-axis or y-axis. No, it will not be classified as an outlier.   Question 10: You encountered a data set in which all variables are normally distributed with an unequal variance and unequal expectancy (mean). You wish to run a KMeans clustering to cluster the data. What would you do as a preprocessing step?\nScale and center the data using the function scale. Scale and center the data using min-max scaling and centering. Either a or b. Nothing - since the data is already normally distributed, no scaling or centering is required.   Bonus question (5 points bonus): Did you sign up for R-Bloggers updates? (feed to receive R related news and updates)\nYes (5 points bonus). No, but I‚Äôm doing it now (2.5 points bonus). No, and I don‚Äôt intend to.  P.S. I‚Äôm not getting any benefits from R-bloggers for ‚Äúadvertising‚Äù them, I genuinly think it‚Äôs a great source to stay updated, and improve your R capabilities.\n Quiz answers Answers available in the following gist.\n ","date":1556452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556452800,"objectID":"020dbfb01905c730d5ee58584a07dfe6","permalink":"https://adisarid.github.io/post/2019-04-28-test_your_tidyness/","publishdate":"2019-04-28T12:00:00Z","relpermalink":"/post/2019-04-28-test_your_tidyness/","section":"post","summary":"Over the last month I gave a tidyverse + intro to data science corporate training in a startup in Tel-Aviv. We had two groups (beginners and intermediates), and for the last assignment of the course I was aiming for a short quiz comprised of various topics which we covered during the course, such that can also be automated easily (i.","tags":["Tidyverse"],"title":"Test your tidyness - a short quiz to check your tidyverse capabilities","type":"post"},{"authors":["admin"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://adisarid.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":null,"categories":["R"],"content":"  A few months ago I attended the 2019 rstudio::conf, including the shiny train-the-trainer workshop. It was a two day workshop and it inspired me in many ways. The first day of the workshop focused on the very basics of teaching (R or anything else), and for me it put the spotlight on things I never considered before.\nOne of the important takeways from the workshop was how to approach educating others: preparing for a course, things you can do during the lessons, and how to self-learn and improve my own teaching methods afterwards.\nThis led me to create the teachR‚Äôs cheatsheet. It outlines the basics of teaching and I chose to give it the flavour of R (in the examples and illustrations within the cheatsheet).\nI have contributed it to RStudio‚Äôs cheat sheet repo, so you can download it directly from: https://github.com/rstudio/cheatsheets/raw/master/teachR.pdf.\nIn the cheat sheet you will find three segments:\nPreparing a new course / workshop / lesson. Things you can do during the lesson itself. Things you should do when the course is completed in order to improve your own teaching methods.  I previously blogged about some of the things learned at the train-the-trainer, and not everything made it to the cheat sheet, so if you‚Äôre interested you can read more here.\nHere‚Äôs an example for some of the things you can find in the cheat sheet.\nDesigning a new course The cheat sheet covers the various steps of designing a course, i.e.:\nPersona analysis of your learners. Defining the course‚Äôs goals using Bloom‚Äôs taxonomy. Using conceptual maps to grasp what the the course should look like and what related terms/materials should appear. Writing the final exam, the slides, check-ins and faded examples.  Here are some examples relating to 1-2:\nPersona analysis Take a while to understand and characterize your learners: are the novice? advanced? false experts?\nWhat are the learner‚Äôs goals from the course? what prior knowledge you can assume (and what not), and do they have any special needs.\nIf end up with too many personas anticipate trouble - it‚Äôs hard to accomodate a diverse crowd, what are you going to miss out on?\n Define goals using Bloom‚Äôs taxonomy Bloom‚Äôs taxonomy illustrates the levels of learning new concepts or topics.\nThe Vanderbilt University Center for Teaching has a nice illustration for it.\nBlooms Taxonomy\n You can visit the Vanderbilt website for a more thorough explanation about the taxonomy, but suffice it to say that ‚Äúremember‚Äù is the most basic form of acquired knowledge, and the highest levels (at the top of the pyramid) are evaluate and create (being able to evaluate someone else‚Äôs work, or create your own noval work).\nIf we translate that to R, ‚Äúremember‚Äù might translate to: ‚Äúlearners will be able to state the main packages in tidyverse and their purpose‚Äù versus ‚Äúcreate‚Äù which in that context would translate to: ‚Äúlearners will be able to contribute to a tidyverse packages or create their own tidy package.‚Äù You can see that the first is something you can teach an R beginner but the latter is much more complex and can be mastered by an advanced useR.\nWorking with Bloom‚Äôs taxonomy can help you set your goals for the course and also help you set the expectations with the learners of your course.\n  During the course Some tips I learned at the train-the-trainer workshop, for when you are during the lesson itself.\nSticky notes At the start of the lesson, give each learner three sticky notes (green, red, and blue). The learners put them on their computers according to their progress:\n Green = I‚Äôm doing fine / finished teh exercise. Red = Something is wrong, I need help. Blue = I need a break  If you see a lot of greens - try to up the pace. If you see a lot of reds, maybe take it easier.\n Check-ins Try to set a few check-ins every hour, to evaluate the progress and make sure that the learners are ‚Äúwith you‚Äù. You can even use some kind of online surveying tool to turn this into a ‚Äúgame‚Äù.\n  After the course Make sure you debrief properly, and learn from your experience. Use surveys to collect feedback. Also measure the time each chapter really takes you, so you can better estimate the time for each type of lesson.\n Conclusion Teaching can be challenging, but it is also rewarding and fun.\nIt is important to come well prepared, and this cheat sheet can help you checklist what you need to do: https://github.com/rstudio/cheatsheets/raw/master/teachR.pdf\nTeaching is an iterative process in which you can keep improving each time, if you measure and learn from your mistakes.\n ","date":1552392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552392000,"objectID":"f3020499e5b9473f9ae7cd815a42bb13","permalink":"https://adisarid.github.io/post/2019-03-12-the_teachr_cheat_sheet/","publishdate":"2019-03-12T12:00:00Z","relpermalink":"/post/2019-03-12-the_teachr_cheat_sheet/","section":"post","summary":"A few months ago I attended the 2019 rstudio::conf, including the shiny train-the-trainer workshop. It was a two day workshop and it inspired me in many ways. The first day of the workshop focused on the very basics of teaching (R or anything else), and for me it put the spotlight on things I never considered before.","tags":["Train the Trainer","Teaching"],"title":"The teachR's::cheat sheet","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://adisarid.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["R"],"content":"  A few days ago I presented at the 9th Israeli class action lawsuit conference. You‚Äôre probably asking yourself what would a data scientist do in a room full of lawyers?\nApparently, there is a lot to do‚Ä¶ Here‚Äôs the story: being in market research, we get a lot of lawyers which are faced with class action lawsuits (either suing or being sued) - and they hire us to conduct research and estimate things like the size of the group for the class action, or the total damages applied on the group.\nThis time, we did something special. we conducted our own survey, with consumers in the general public in Israel. The goal was to rate various ways of getting compensation (after settling a class action lawsuit).\nFor that we used conjoint analysis. Conjoint is where you ask the survey participants a set of questions (five in our case). Each question has a number of alternatives (or packages) to choose from, and these are randomized per respondent. In our case we showed three packages, each package is defined by three parameters relating to how a consumer can get compensation in case of a class action being won:\nPush versus pull - do you have to ask for the compensation or would you get the notification/compensation without asking. The value of the compensation - tested at 4 levels (25, 50, 75, and 100 ILS) The method of delivery - as a complimentary product, a refund at next purchase, bank cheque, or credit card.  The thing about conjoint analysis is that when you diversify enough, you can then run various models to estimate the weight of each parameter, i.e., using logistic regression.\nThe data is available in the github repo, and the specific data is under the data folder.\n#library(tidyverse) class_action_conjoint \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/adisarid/class-action-IL-survey/master/data/20190130020529-SurveyExport-general_public-conjoint.csv\u0026quot;, skip = 1, col_names = c(\u0026quot;Response ID\u0026quot;, \u0026quot;Set Number\u0026quot;, \u0026quot;Card Number\u0026quot;, \u0026quot;compensation_push_pull\u0026quot;, \u0026quot;compensation_amount_ILS\u0026quot;, \u0026quot;compensation_type\u0026quot;, \u0026quot;score_selection\u0026quot;)) ## ## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ## cols( ## `Response ID` = col_double(), ## `Set Number` = col_double(), ## `Card Number` = col_double(), ## compensation_push_pull = col_character(), ## compensation_amount_ILS = col_double(), ## compensation_type = col_character(), ## score_selection = col_double() ## ) glimpse(class_action_conjoint) ## Rows: 7,020 ## Columns: 7 ## $ `Response ID` \u0026lt;dbl\u0026gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10‚Ä¶ ## $ `Set Number` \u0026lt;dbl\u0026gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 1‚Ä¶ ## $ `Card Number` \u0026lt;dbl\u0026gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1‚Ä¶ ## $ compensation_push_pull \u0026lt;chr\u0026gt; \u0026quot;push\u0026quot;, \u0026quot;push\u0026quot;, \u0026quot;push\u0026quot;, \u0026quot;push\u0026quot;, \u0026quot;pull\u0026quot;, \u0026quot;pull\u0026quot;‚Ä¶ ## $ compensation_amount_ILS \u0026lt;dbl\u0026gt; 75, 50, 25, 100, 25, 100, 75, 100, 50, 25, 75,‚Ä¶ ## $ compensation_type \u0026lt;chr\u0026gt; \u0026quot;another_product\u0026quot;, \u0026quot;credit_cart\u0026quot;, \u0026quot;bank_cheque‚Ä¶ ## $ score_selection \u0026lt;dbl\u0026gt; 0, 0, 100, 0, 100, 0, 0, 0, 100, 0, 100, 0, 10‚Ä¶ class_action_conjoint %\u0026gt;% count(compensation_push_pull) ## # A tibble: 2 x 2 ## compensation_push_pull n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 pull 3516 ## 2 push 3504 class_action_conjoint %\u0026gt;% count(compensation_amount_ILS) ## # A tibble: 4 x 2 ## compensation_amount_ILS n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 25 1759 ## 2 50 1761 ## 3 75 1752 ## 4 100 1748 class_action_conjoint %\u0026gt;% count(compensation_type) ## # A tibble: 5 x 2 ## compensation_type n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 another_product 1391 ## 2 bank_cheque 1410 ## 3 coupon 1403 ## 4 credit_cart 1415 ## 5 refund_next_purchase 1401 You can see that the different options are balanced (they should be - they were selected randomly) and that the number of observations is \\(7,020\\). This is because we had \\(n=468\\) respondents answering the conjoint question groups, each selecting best one out of three, with five such random sets (\\(5*3*468=7020\\)).\nLogistic regression The easiest (and most basic) way to start analyzing the conjoint data is with logistic regression. Note that I‚Äôm not endorsing this use of logistic regression in conjoint analysis, because nowadays it has become a standard to compensate for mixed effects (see package lme4). However, for the purposes of this post, I‚Äôm going to carry on with the simple glm which is sufficiently good for our illustration. In any case, my experience is that the models yield similar results in most cases.\nglm_set \u0026lt;- class_action_conjoint %\u0026gt;% mutate(score_selection = score_selection/100) %\u0026gt;% mutate(compensation_push_pull = factor(compensation_push_pull, levels = c(\u0026quot;pull\u0026quot;, \u0026quot;push\u0026quot;), ordered = F), compensation_type = factor(compensation_type, levels = c(\u0026quot;another_product\u0026quot;, \u0026quot;refund_next_purchase\u0026quot;, \u0026quot;coupon\u0026quot;, \u0026quot;bank_cheque\u0026quot;, \u0026quot;credit_cart\u0026quot;), ordered = F)) %\u0026gt;% select(-`Set Number`, -`Card Number`, -`Response ID`) %\u0026gt;% mutate(compensation_amount_ILS = factor(compensation_amount_ILS, levels = c(25, 50, 75, 100))) conjoint_glm_model \u0026lt;- glm(data = glm_set %\u0026gt;% select(score_selection, compensation_push_pull, compensation_amount_ILS, compensation_type), formula = score_selection ~ ., family = binomial()) summary(conjoint_glm_model) ## ## Call: ## glm(formula = score_selection ~ ., family = binomial(), data = glm_set %\u0026gt;% ## select(score_selection, compensation_push_pull, compensation_amount_ILS, ## compensation_type)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8635 -0.8111 -0.5139 0.8796 2.6557 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -3.49667 0.11341 -30.832 \u0026lt;2e-16 *** ## compensation_push_pullpush 0.74140 0.05807 12.767 \u0026lt;2e-16 *** ## compensation_amount_ILS50 1.01476 0.09385 10.813 \u0026lt;2e-16 *** ## compensation_amount_ILS75 1.73623 0.09224 18.823 \u0026lt;2e-16 *** ## compensation_amount_ILS100 2.40149 0.09281 25.876 \u0026lt;2e-16 *** ## compensation_typerefund_next_purchase 0.08431 0.10399 0.811 0.418 ## compensation_typecoupon 1.10396 0.09588 11.514 \u0026lt;2e-16 *** ## compensation_typebank_cheque 1.53888 0.09473 16.245 \u0026lt;2e-16 *** ## compensation_typecredit_cart 1.89640 0.09586 19.782 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8936.7 on 7019 degrees of freedom ## Residual deviance: 7295.6 on 7011 degrees of freedom ## AIC: 7313.6 ## ## Number of Fisher Scoring iterations: 4 Note how most variables (actually all but compensation_typerefund_next_purchase) are significant and with a positive estimate (i.e., odds ratio \u0026gt; 1). This means means that when a certain variable increases, the probability of choosing the package increases, i.e.:\n Passively getting the compensation (‚ÄúPush‚Äù) is better than a required act to get the compensation (‚Äúpull‚Äù) . Any sum of money (50, 75, 100) is better than 25, in an increasing odds ratio. Most compensation types (credit card payback, bank cheque, coupon) are significantly better than a complimentary product.  Now comes the interesting part: for example, compare the following three packages. Try to guess which one is more attractive:\n  Parameter Package 1 Package 2 Package 3    Push/Pull Pull Pull Push  Return Credit Refund Coupon  Price 25 75 25    It is not that easy to determine between the three. In this situation there is no single strategy which is superior to the others, we can however plot these three packages with the logistic regression response and standard errors. First let‚Äôs put them all in a tibble (I also added the best and worst packages).\npackage_comparison \u0026lt;- tribble( ~package_name, ~compensation_push_pull, ~compensation_amount_ILS, ~compensation_type, \u0026quot;pkg1\u0026quot;, \u0026quot;pull\u0026quot;, 25, \u0026quot;credit_cart\u0026quot;, \u0026quot;pkg2\u0026quot;, \u0026quot;pull\u0026quot;, 75, \u0026quot;refund_next_purchase\u0026quot;, \u0026quot;pkg3\u0026quot;, \u0026quot;push\u0026quot;, 25, \u0026quot;coupon\u0026quot;, \u0026quot;worst\u0026quot;, \u0026quot;pull\u0026quot;, 25, \u0026quot;another_product\u0026quot;, \u0026quot;best\u0026quot;, \u0026quot;push\u0026quot;, 100, \u0026quot;credit_cart\u0026quot; ) %\u0026gt;% mutate(compensation_amount_ILS = factor(compensation_amount_ILS)) # need to convert to factor - which is how it is modeled in the glm. predicted_responses \u0026lt;- predict(conjoint_glm_model, newdata = package_comparison, type = \u0026quot;response\u0026quot;, se.fit = T) # lets join these together package_responses \u0026lt;- package_comparison %\u0026gt;% mutate(fit = predicted_responses$fit, se.fit = predicted_responses$se.fit) package_responses ## # A tibble: 5 x 6 ## package_name compensation_pu‚Ä¶ compensation_am‚Ä¶ compensation_ty‚Ä¶ fit se.fit ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 pkg1 pull 25 credit_cart 0.168 0.0129 ## 2 pkg2 pull 75 refund_next_pur‚Ä¶ 0.158 0.0122 ## 3 pkg3 push 25 coupon 0.161 0.0129 ## 4 worst pull 25 another_product 0.0294 0.00324 ## 5 best push 100 credit_cart 0.824 0.0123 # P.S. - excuse the \u0026quot;credit_cart\u0026quot; typo (I build the model that way, only then noticed...) ggplot(package_responses %\u0026gt;% slice(1:3) , aes(x = package_name, y = fit)) + geom_point() + geom_errorbar(aes(ymin = fit - se.fit, ymax = fit + se.fit)) + ggtitle(\u0026quot;Package comparison (packages 1-3)\u0026quot;, subtitle = \u0026quot;Error bars represent the SE\u0026quot;) + ylab(\u0026quot;Predicted response (glm logit)\u0026quot;) + xlab(\u0026quot;Package name\u0026quot;) + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) ggplot(package_responses, aes(x = package_name, y = fit)) + geom_point() + geom_errorbar(aes(ymin = fit - se.fit, ymax = fit + se.fit)) + ggtitle(\u0026quot;Package comparison (including best and worst packages)\u0026quot;, subtitle = \u0026quot;Error bars represent the SE\u0026quot;) + ylab(\u0026quot;Predicted response (glm logit)\u0026quot;) + xlab(\u0026quot;Package name\u0026quot;) + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) We see that the three packages (pkg1, pkg2, and pkg3) are relatively similar, within one standard error from one another. When compared to the worst package they are roughly \\(\\sim8\\) times better (via odds ratio), but the best package is \\(\\sim5\\) times better than packages 1-3.\nOne can use these concepts to illustrate the benefits of each parameter on the different packages, and let the user experience how different features make the packages more or less ‚Äúattractive‚Äù.\nAs an experiment, I prepared a nice little shiny app which lets the user experiment with the different features: build two packages and then compare them. You can checkout the code at the github repo, or check out the live app here.\n Conclusions Surveys are a popular tool used in class actions (at least in Israel). They can be used to estimate the tradeoffs between various types of compensation or settlement, for example with the use of conjoint analysis.\nWith a glm model one can tell the differences of various packages, and the odds ratio is a way to illustrate to decision makers a comparison of various options (and how much ‚Äúmore attractive‚Äù is one package over another).\nA shiny app can be a nice way to illustrate the results of a conjoint analysis, and to let the user experiment with how different features make a specific option better or worse than another option.\n ","date":1549195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549195200,"objectID":"afd35131d0c5babf2c2f856fccadaf41","permalink":"https://adisarid.github.io/post/2019-02-03-class-action-conjoint/","publishdate":"2019-02-03T12:00:00Z","relpermalink":"/post/2019-02-03-class-action-conjoint/","section":"post","summary":"A few days ago I presented at the 9th Israeli class action lawsuit conference. You‚Äôre probably asking yourself what would a data scientist do in a room full of lawyers?","tags":["Conjoint analysis","Class actions","shiny"],"title":"Settling class action lawsuits with conjoint analysis and R (+a conjoint shiny app)","type":"post"},{"authors":null,"categories":["R"],"content":"  With all the functional programming going on (i.e., purrr::map and the likes), there is at least one thing that I found missing: progress bars. The plyr::do function had a nice looking progress bar open up by default if the operation took more than 2 seconds and had at least two more to go (as per Hadley‚Äôs description in Issue#149 in tidyverse/purrr).\nThe issue is still open, for the time of writing these lines, and will probably be solved sometime in the near future as a feature of purrr::map.\nPersonally, I like @cderv‚Äôs elegent solution suggested at that same github issue.\nHere is an example implementation for reading multiple files within a directory and combining them into a single tibble while showing a progress bar when reading the files. The file reading is very similar to what was suggested in this post.\nlibrary(purrr) library(readr) library(dplyr) # directory from which to read a bunch of files (the example here uses csv) file_list \u0026lt;- dir(path = \u0026quot;PATH_TO_DIRECTORY\u0026quot;, pattern = \u0026quot;.csv\u0026quot;) # define reading function which includes the progress bar updates and printing read_with_progress \u0026lt;- function(filename){ pb$tick()$print() data_read \u0026lt;- read_csv(filename) # you can add additional operations on data_read, or # decide on entirely different task that this function should do. } # create the progress bar with a dplyr function. pb \u0026lt;- progress_estimated(length(file_list)) res \u0026lt;- file_list %\u0026gt;% map_df(~read_with_progress(.)) That‚Äôs it. You‚Äôre set to go with a cool progress bar which will print out something like this while the operation is carried out:\n|===================================== |80% ~23 s remaining ","date":1548417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548417600,"objectID":"afa1fe5368f57b1caff3bb0279096363","permalink":"https://adisarid.github.io/post/2019-01-24-purrrying-progress-bars/","publishdate":"2019-01-25T12:00:00Z","relpermalink":"/post/2019-01-24-purrrying-progress-bars/","section":"post","summary":"With all the functional programming going on (i.e., purrr::map and the likes), there is at least one thing that I found missing: progress bars. The plyr::do function had a nice looking progress bar open up by default if the operation took more than 2 seconds and had at least two more to go (as per Hadley‚Äôs description in Issue#149 in tidyverse/purrr).","tags":["purrr","progress bars"],"title":"Purrring progress bars (adding a progress bar to `purrr::map`)","type":"post"},{"authors":null,"categories":["R"],"content":"  Following Jenny Bryan‚Äôs talk on tidyeval in the last rstudio::conf 2019, I decided to write this short note (mainly as a reminder to myself).\nWhat is tidyeval? Tidy evaluation, or non standard evaluation, allows us to pass column names between functions. This is the ‚Äúclassic‚Äù behaviour of most tidyverse functions. For example, we use:\nlibrary(tidyverse) mtcars %\u0026gt;% select(mpg, cyl) ## mpg cyl ## Mazda RX4 21.0 6 ## Mazda RX4 Wag 21.0 6 ## Datsun 710 22.8 4 ## Hornet 4 Drive 21.4 6 ## Hornet Sportabout 18.7 8 ## Valiant 18.1 6 ## Duster 360 14.3 8 ## Merc 240D 24.4 4 ## Merc 230 22.8 4 ## Merc 280 19.2 6 ## Merc 280C 17.8 6 ## Merc 450SE 16.4 8 ## Merc 450SL 17.3 8 ## Merc 450SLC 15.2 8 ## Cadillac Fleetwood 10.4 8 ## Lincoln Continental 10.4 8 ## Chrysler Imperial 14.7 8 ## Fiat 128 32.4 4 ## Honda Civic 30.4 4 ## Toyota Corolla 33.9 4 ## Toyota Corona 21.5 4 ## Dodge Challenger 15.5 8 ## AMC Javelin 15.2 8 ## Camaro Z28 13.3 8 ## Pontiac Firebird 19.2 8 ## Fiat X1-9 27.3 4 ## Porsche 914-2 26.0 4 ## Lotus Europa 30.4 4 ## Ford Pantera L 15.8 8 ## Ferrari Dino 19.7 6 ## Maserati Bora 15.0 8 ## Volvo 142E 21.4 4 The two variables were selected out of the mtcars data set, and we specified them as names without using any quotation marks. They are symbolic, not characters (although they could also be specified as characters, select is smart enough that way).\nBut assume we want to pass variables ‚Äútidy style‚Äù between functions which do different operations.\n Variation one - a basic example We‚Äôll start simple: a function which has two parameters. The first parameter is a dataset. The second parameters is a grouping variable. All other variables in the data set will have their mean computed using summarize_all.\ntest1 \u0026lt;- function(dataset, groupby_vars){ grouping_vars \u0026lt;- enquo(groupby_vars) dataset %\u0026gt;% group_by(!! grouping_vars) %\u0026gt;% summarize_all(funs(mean(.))) %\u0026gt;% return() } mtcars %\u0026gt;% select(cyl:carb) %\u0026gt;% test1(groupby_vars = cyl) ## Warning: `funs()` was deprecated in dplyr 0.8.0. ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## # A tibble: 3 x 10 ## cyl disp hp drat wt qsec vs am gear carb ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4 105. 82.6 4.07 2.29 19.1 0.909 0.727 4.09 1.55 ## 2 6 183. 122. 3.59 3.12 18.0 0.571 0.429 3.86 3.43 ## 3 8 353. 209. 3.23 4.00 16.8 0 0.143 3.29 3.5 We can see that mtcars was grouped by cyl which was passed as a name (not characters). The function test1 took it, then enquo()-ed it, and eventually used it in the tidy chain using !!. The function enquo turns the input into a ‚Äúquosure‚Äù. Then the !! ‚Äúuses‚Äù the quosure to select the proper variable from mtcars.\n Passing arguments using ... A slightly more complex situation is passing multiple arguments to the function. Assume that this time we want to construct a function which gets one input by which to group by, and what are the variables to be summarized:\ntest2 \u0026lt;- function(dataset, groupby_vars, ...){ grouping_vars \u0026lt;- enquo(groupby_vars) dataset %\u0026gt;% group_by(!! grouping_vars) %\u0026gt;% summarize_at(vars(...), funs(mean(.))) %\u0026gt;% return() } mtcars %\u0026gt;% select(cyl:carb) %\u0026gt;% test2(groupby_vars = cyl, disp:drat) ## # A tibble: 3 x 4 ## cyl disp hp drat ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4 105. 82.6 4.07 ## 2 6 183. 122. 3.59 ## 3 8 353. 209. 3.23 What happend is that test2 treats the grouping variable the same way that test1 treated it, but it also passed along the variables disp:drat.\n Maximum flexibility - multiple enquo()s Sometime passing the dots, i.e., ... is not enough. For example, if we want specify behaviour for different columns of the data frame (e.g., compute the mean for some and the std for others). In such cases we need a more flexible version. We can extend the flexibilty of this approach using multiple enqou()s.\ntest3 \u0026lt;- function(dataset, groupby_vars, computemean_vars, computestd_vars){ grouping_vars \u0026lt;- enquo(groupby_vars) mean_vars \u0026lt;- enquo(computemean_vars) std_vars \u0026lt;- enquo(computestd_vars) dataset %\u0026gt;% group_by(!! grouping_vars) %\u0026gt;% summarize_at(vars(!!mean_vars), funs(mean(.))) %\u0026gt;% left_join(dataset %\u0026gt;% group_by(!! grouping_vars) %\u0026gt;% summarize_at(vars(!!std_vars), funs(sd(.)))) } mtcars %\u0026gt;% test3(groupby_vars = cyl, disp:drat, wt:carb) ## Joining, by = \u0026quot;cyl\u0026quot; ## # A tibble: 3 x 10 ## cyl disp hp drat wt qsec vs am gear carb ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4 105. 82.6 4.07 0.570 1.68 0.302 0.467 0.539 0.522 ## 2 6 183. 122. 3.59 0.356 1.71 0.535 0.535 0.690 1.81 ## 3 8 353. 209. 3.23 0.759 1.20 0 0.363 0.726 1.56 In the resulting table, the first column cyl is the grouping variable, columns disp through drat have the mean of the corresponding variables, and columns wt through carb have their standard deviation computed.\n Additional uses of tidy evaluation This evaluation is very useful when building flexible functions, but also when using the ggplot2 syntax within functions, and more so when using Shiny applications, in which input parameters need to go in as grouping or as plotting parameters.\nHowever, this is a topic for a different post.\n Conclusions Tidy evaluation empowers you with great tools - it offers a great degree of flexibilty, but it‚Äôs a bit tricky to master.\nMy suggestion is that if you‚Äôre trying to master tidy evaluation, just think about your use case: which of the three variations presented in this post it resembles too?\nWork your way up - from the simplest version (if it works for you) and up to the complex (but most flexible) version.\n ","date":1547985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547985600,"objectID":"bffee4df110f883415ea043910de9482","permalink":"https://adisarid.github.io/post/2019-01-20-short-note-about-tidy-eval/","publishdate":"2019-01-20T12:00:00Z","relpermalink":"/post/2019-01-20-short-note-about-tidy-eval/","section":"post","summary":"Following Jenny Bryan‚Äôs talk on tidyeval in the last rstudio::conf 2019, I decided to write this short note (mainly as a reminder to myself).\nWhat is tidyeval? Tidy evaluation, or non standard evaluation, allows us to pass column names between functions.","tags":["tidyeval"],"title":"Short note about tidyeval","type":"post"},{"authors":null,"categories":["R"],"content":"  First, let me start by saying wow!, what a wonderful experience.\nWhen I booked the trip from Israel to Austin, TX, I thought that I‚Äôll see some good content, and learn at the conference (as I in fact did). It was much more enjoyable than I could‚Äôve imagined. In part I guess this can be contributed to the awesome R community. The ease in which you start a conversation with just about anyone in the conference - about R, professional life (or even personal life), that‚Äôs great.\nBesides that, visiting Texas (for the first time) was interesting, but for more on that - see venue.\nWorkshop ‚ÄúShiny Train-the-Trainer‚Äù This was a two day workshop. The first day was taught by Greg Wilson, and touched different points of teaching in general, and teaching programming lanugages. The second day was taught by Mine, and zoomed-in on Shiny apps and how to teach building Shiny apps.\nStarting from the theory and building up, Greg was very charismatic. For me, some takeaways from the workshop were:\n Each time you start a new course, choose 1-2 things you want to improve/tryout. Don‚Äôt try to go ‚Äúall-in‚Äù because then you might miss. Also, make sure that when you do implement new techniques, you don‚Äôt fall short on things you were doing so far which were good.  This is going to be slightly cumbersome (I‚Äôm also summarizing this for my own good).\nA few things that helps organize and conduct sessions.\nFigure out who are your learners Figure out who are your learners: what are they interested in? what do they already know, and what they don‚Äôt know. What is the diversity you‚Äôre going to get in the crowd (persona analysis).\nHere‚Äôs are examples for different personas we cooked up during the workshop.\nA shiny novice  Background: Statisticians and Data Scientists from the pharma industry. Prior knowledge: Some knowledge but not state of the art. Motivation: Want to build shiny apps to share information with other functions in the company. How the course will help them: Able to build simple shiny apps and grow from there. Special needs: ‚ÄúThink they know but actually don‚Äôt‚Äù.   A shiny expert  Background: New company moving from a start-up. Biostatistics PhD. Prior knowledge: Used R, made apps for paper presentations, done some shiny apps. Looks good but they know that there are stuff that they don‚Äôt know. Mix of formal (for the statistics) but a lot of fun learning on their own. Motivation: Utilize what they have done, but in the context of the organization, and learn about new and cool things. How the course will help them: Teach them the newest and best things of Shiny, in the context of the company. This is how make it production and enterprise ready. Special needs: Work from home. Interaction remote. Dog that barks. Online course.   A student you expect to encounter at a shiny workshop you teach  Background: Danny is studying industrial engineering. Undergrad. He is in his third year, and about to finish next year. He likes data science and likes aquiring new programming skills. Prior knowledge: Danny has learned some python and some base R, throghout the last few semsters, but he is not fluent in either. Still struggling with some commands in R. Motivation: Danny wants to aquire a new tools that will help him next year when he looks for new work, and will help him impress potential employers. How the course will help them: Danny will be able to build apps and use them as showcase while he is looking for work next year. In addition, he will be able to build shiny apps that will help him publish and distribute findings. Special needs: Danny has a lot of motivation, but is a novice to R and programming in general. He doesn‚Äôt have a lot of time for exercise because the semster courses take up a lot of his time, nonetheless, he is willing to invest the time in projects at specific ‚Äúpeaks‚Äù in order to advance his skills.    Write the learning objectives Write learning objectives which are observable (by the learners) and also measureable. For example:\nThe student will build a shiny app that does‚Ä¶ The students will learn and use the renderPlot() function, etc.  Here‚Äôs another example for learning objectives of a short introductory 1-hour session:\nThe students will understand the basic elements of a shiny app and describe the difference between ui, server and global. The students will apply the princibles to modify an example reading a file and showing a table with the first 10 lines of the file. The students will learn about and be aware of shiny examples in the gallery.  To formulate the objectives, one can use bloom‚Äôs taxonomy (and the extended version of it that Mine have shown during class).\n Build conceptual maps Build concept maps for each of the topics in the course. I.e., for each class there is a concept map that highlights the topics and the connections between them. Again, referring to the steps in Bloom‚Äôs taxonomy as the building blocks of those building blocks. While building the concept maps, remmember that:\n Roughly 5-9 items can fit in the short term memory. Consider that when building the concept map. Make sure it‚Äôs not too complicated. To make things slightly easier, while teaching, you can use a whiteboard to expand the concept map and show the class where we are on the concept map.   Write the ‚Äúfinal exam‚Äù and formative assesments Write the final exam, i.e., in the end of the session, what should they be able to answer? This should correspond to the aferomentioned learning objectives.\nGenerate formative assesments (short questions for ‚Äúcheck-ins‚Äù), that will be used during the lesson. These will help you check if the crowd is with you or lost got lost.\n Create the presentation and learning materials Finally, create the learning material around the previous steps. Re-iterate as needed to improve the materials.\n Some more useful tools and tips  Sticky notes are very useful during programming lessons. This way you can see during the lesson where the class is at. Have 4 colors:  Green = ‚Äúeverything is fine‚Äù Red = ‚Äúneed help‚Äù Blue = ‚Äúwant a break‚Äù Orange = ‚Äúwant to ask a question‚Äù  For ‚Äúcheck-ins‚Äù You can use poll everywhere or a similar solution. Interactions between students are very useful. Ask a question. Let them talk with one another to get the answer. ‚ÄúBaby steps‚Äù - use faded examples or incremental examples when teaching. Avoid a ‚Äúnovice blank page‚Äù when starting. Encourage the students: ‚Äúwhat would you type into stackoverflow to find a solution to this problem?‚Äù No opting-out. If someone doesn‚Äôt know the answer, ask someone else - then go back to that person and ask something else. No one will ‚Äúfall asleep‚Äù, you make sure that everyone are with you. Jump between locations in class (not in the sitting order). Use rstudio.cloud when a uniform R environment is desired. We can even start from a flat base instance with all the packages pre-loaded.   Feedback  Choose an element from above. Use feedback to understand if it was good or not. Choose an existing thing you do and check that you didn‚Äôt lose it either. Encourage the students to give feedback to one another by interacting in exercises (can scale up to larger classes).   Further reading Check the online materials of the course. Everything is on a creative commons - BY RStudio - license.\nAll resources for teaching techniques are available at Greg Wilson‚Äôs website teachtogether.tech.\nResources specific for Shiny teaching are available at the workshop‚Äôs website, including Mine‚Äôs teaching notes for 1hr, 2hr, 1/2day, 1day, 2day workshops. See http://teach-shiny.rbind.io.\n  Conference Day 1 Here are some highlights:\n Joe Chang‚Äôs keynote: lots of tools for testing and profiling shiny apps. Speed improvments for shiny apps using cache (plotCacheRender()). Showd some techniques to make apps much quicker. API development with R and Tensor flow at T-mobile: A really cool use case for using shiny apps and plumber API. This is an example for scaling up a plumber api for a customer facing app. Databases using R: the latest: Edgar demonstrated how he connects to a google big query database. The big query server does all the computations and the clean (and smaller data) is input into R for continued analysis. Very cool. Working with categorial data in R without loosing your mind: some best practices for working with factors. Advocating forecats (actually much of these I already implement in my work anyhow). Melt the clock: tidy time series analysis: a talk about tsibble and fable, the packages which are about to replace the forecast and ts(), in a tidy-er version. Can be installed from github and should make life much easier for time series analysis. 3D mapping, plotting, and printing with rayshader: an extremely cool package for ploting maps from x-y-z (elevation data). A lot of options, and even includes an export for 3d printing of the models. gganimate Live Cookbook: A nice package for animating ggplots. But need to carefully choose when to use and when not to use it‚Ä¶   Conference Day 2 Great RMarkdown session - kind of made me rethink about how I do my work. For me personally a lot of the work envolves power point and word, but from now on, I think I‚Äôll try to do it on RMarkdown. More reproduceable, easier to recreate, or update if needed.\n Yihui Xie talked about blogdown, bookdown, and more recently pagedown (for academic publications). I also came to know about Radix - a package for Academic style blogs. In the new RStudio (version 1.2 and above, currently in preview), there is an option to export RMarkdown documents into power point presentation. Works seamlessly, just change the yaml at the top:  --- title: \u0026quot;This is a power point presentation\u0026quot; author: \u0026quot;I\u0026#39;m the author\u0026quot; date: \u0026quot;...\u0026quot; output: powerpoint_presentation ---  The gt package was presented. A package which is great for generating html or \\(\\LaTeX\\) tables. A cool feature of RMarkdown which I didn‚Äôt know about is parameters. It enables to creat variants of an RMarkdown document without actually changing the document. For more info see parameterized reports. Hadley Wikam talked about a new package in development vctrs that is supposed to improve type consistency in R. Here‚Äôs an example with factor and c‚Äôs default behaviour vs.¬†the vec_c solution in vctrs:  c(factor(\u0026quot;a\u0026quot;), factor(\u0026quot;b\u0026quot;)) ## [1] 1 1 vctrs::vec_c(factor(\u0026quot;a\u0026quot;), factor(\u0026quot;b\u0026quot;)) ## [1] a b ## Levels: a b  Jenny Bryan talked about lazy evaluation. Finally made some sense to me about when to use enquo() and when to use !!. Another good talk about the ipc package for Shiny apps which require heavy and async computations. That package is able to pass queue and interrupt messages between processes. David Robinson from DataCamp gave a really inspiring talk about what you can and should do openly (hence me writing this blog post, in the hopes that it‚Äôll be the first of many).   Venue I landed on Monday and the workshops started on Tuesday, so not too much time to hand around, but following the recommendation of the receptionist in my hotel (which was great, Holiday Inn Austin Town Center), I went to Terry Black‚Äôs - an original BBQ resteraunt, which was very good (apparantly also very high on TripAdvisor).\nI also checked out the South of Colorado (SoCo) area - it was nice to walk around and by some small things for the wife and kids.\nParks around the Colorado River - real nice to walk around or jog.\n Takeaways  Use more RMarkdown and less powerpoint/word. Lot of tips on how to improve my R courses (which I should implement). Shiny is extremly powerful, much more than what I‚Äôm using today. Should probably find the time to improve my own Shiny building/programming skills. Do a lot more blogging with R and blogdown.   ","date":1547899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547899200,"objectID":"6f8148aadf5bfaf4a5f5fb1d8d604df3","permalink":"https://adisarid.github.io/post/2019-01-19-rstudio-conf-recap/","publishdate":"2019-01-19T12:00:00Z","relpermalink":"/post/2019-01-19-rstudio-conf-recap/","section":"post","summary":"First, let me start by saying wow!, what a wonderful experience.\nWhen I booked the trip from Israel to Austin, TX, I thought that I‚Äôll see some good content, and learn at the conference (as I in fact did).","tags":["rstudio::conf","conference"],"title":"Recap: what I learned in rstudio::conf2019","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://adisarid.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://adisarid.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["admin","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://adisarid.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["admin","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://adisarid.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://adisarid.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]