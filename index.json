[{"authors":null,"categories":null,"content":"Adi Sarid is a partner and head of Operations Research Department at the Sarid Research Institute LTD.\nAdi is completing his PhD in Operations Research at the department of Industrial Engineering in Tel-Aviv university. Adi holds an MSc in Operations Research from Tel-Aviv university, and a BA in Mathematics Statistics and Operations Research from the Technion.\nAdi is also a certified RStudio instructor, teaching tidyverse, shiny, statistics, and data science.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"213e86da0aa54947cddd303cfc3f5a39","permalink":"/author/adi-sarid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/adi-sarid/","section":"author","summary":"Adi Sarid is a partner and head of Operations Research Department at the Sarid Research Institute LTD.\nAdi is completing his PhD in Operations Research at the department of Industrial Engineering in Tel-Aviv university. Adi holds an MSc in Operations Research from Tel-Aviv university, and a BA in Mathematics Statistics and Operations Research from the Technion.\nAdi is also a certified RStudio instructor, teaching tidyverse, shiny, statistics, and data science.","tags":null,"title":"","type":"author"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"d41d8cd98f00b204e9800998ecf8427e","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"author","summary":"","tags":null,"title":"Authors","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536440400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536440400,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00+03:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":["R"],"content":"\rI’ve been building R shiny apps for a while now, and ever since I started working with shiny, it has significantly increased the set of services I offer my clients.\nHere’s a documentations of some of the many lessons I learned in previous projects I did. Hopefully, others can avoid them in the future.\nBackground\rShiny is a really great tool that allows data scientists to communicate their analysis in an appealing and an effective way. However, as data scientists we are used to thinking about things a certain way. Our way of thinking, and our practices are different than these of a software developer or a DevOps.\nHere are some of things I learned along the path of my Shiny app developing experiences - some things that you should and should not do.\n\rDon’t skip the planning phase\rDo a mockup, research your implementation options.\nMockup\rDo a mockup, even if it’s just a piece of paper which took you 5 minutes to draw. It will be worth it.\nShiny is very tempting in the sense that once you understand the concept of reactive programming, you can go from idea to a full app in a few days work. Why invest time in preparing a mockup or planning, when you can just go ahead and do the actual thing?\nMy experience tells me that the app is much more successful in capturing the customer’s needs, when he’s a part of the technical planning phase (when you share your dillemas with the client). It sets expectations, frames what you can and can’t (or won’t) do for the customer, and enables you to find solutions together.\nAlso, when you’re looking at a mockup (even if it’s just a simple drawing or a non-interactive slide), it helps in the next stages of building the app’s UI.\nHere is an example of how a mockup would look like when I’m drawing it on a piece of paper. Note how I’ve already written down the purpose of some of the elements and their expeted element ids. It helps building the UI when you’re actually looking at one of these.\nExample for a mockup drawing\n\r\rResearch\rWhen you encounter a requirement you did not encounter before, and wondering about how to accomplish it, research.\n\rIs there more than a single way to accomplish what you’re trying?\rWhat are the pros and cons of each method?\r\rFor example, when I needed to show a table and incorporate data intake into the table, I was researching two options, one with the DataTable package (via the editable=TRUE argument) and the other is the rhandsontable package.\nBoth provide data editing, eventually I chose randsontable which had some limitations (e.g., slower rendering than DataTable, no search box), but provided more features out-of-the-box (e.g., data validation displaying factors as a list, checkboxes, etc.).\n\r\rBe sure you can live up to your promises\rThis is more of a broad issue (you can say its true for anything).\nIn my case, in the past I promised some clients I’ll provide “realtime” dashboards. However, as it turned out, I was reading from a csv data dump which provided the data with delays going up to 15-30 minutes.\nIn most projects I do, 15 minutes and realtime are pretty much equivalent from a practical standpoint, but in a specific project I did recently, I had a client which wanted to check the data as it was changing minute-by-minute.\nThis gap in expectations caused some confusion and dissappointment. We eventually learned from this, and in the future, when realtime is a requirement, we will use a better data source (i.e., data base instead of the delayed data dump).\n\rDon’t forget to plan your budget\rMake sure you consider all the elements you need for the project. Plan the budget accordingly, and understand the ramifications of scaling the app.\nFor example, if you’re using shinyapps.io, get familiar with the pricing packages, figure out what will you need to provide a good SLA (relative to the number of users of the app).\nSame goes for other cloud services, e.g., using a data base - how many users? how many connections? size of data base?\nIn most cloud providers you can also set up billing alerts which lets you know when something is exceeding a predetermined threshold.\nAll of these are very important when you’re building your quote, and obviously when going into production with your App.\n\rDon’t skip testing and staging on your way to production\rIn software development there are various levels of environments, starting from your desktop (“local”), through development server, integration, testing, staging/acceptance, and production. See Wikipedia’s Deployment environment entry.\nWhen building an app, make sure you go through these steps. Specifically relating to testing, staging, and production). What I found to be particularly useful is to upload the app twice (in two seperate locations/urls):\nDeploy as a beta app (client acceptance/demo) in which I demonstrate additional features and discuss them with the client, before incorporating them into production.\rDeploy as a production/live app.\r\rAs you iterate and improve the app, fix bugs, and add new features, you are also at the risk of breaking things. Thus, you should first update the beta app, share the new additions, and let the client experiment with the app. This way you can double check you didn’t break anything else.\nOnly when the client authorizes the corrections, redeploy the new app to the production.\n\rConclusions\rAs data scientists using Shiny, we’ve also become software developers. We’re developing not just for ourselves or for other useRs in our community.\nWith Shiny we’re building for end-users. We’re building customer facing apps, and we need to keep that in mind. We should make sure that we adopt and use best practices of software development.\n\r","date":1563537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563537600,"objectID":"8d10b47a62962dcb8ce6ad96e0cbfbaa","permalink":"/post/2019-07-03-shiny_app_lessons/","publishdate":"2019-07-19T12:00:00Z","relpermalink":"/post/2019-07-03-shiny_app_lessons/","section":"post","summary":"I’ve been building R shiny apps for a while now, and ever since I started working with shiny, it has significantly increased the set of services I offer my clients.\nHere’s a documentations of some of the many lessons I learned in previous projects I did. Hopefully, others can avoid them in the future.\nBackground\rShiny is a really great tool that allows data scientists to communicate their analysis in an appealing and an effective way.","tags":["Shiny"],"title":"What NOT to do when building a shiny app (lessons learned the hard way)","type":"post"},{"authors":null,"categories":["R"],"content":"\rOver the last month I gave a tidyverse + intro to data science corporate training in a startup in Tel-Aviv. We had two groups (beginners and intermediates), and for the last assignment of the course I was aiming for a short quiz comprised of various topics which we covered during the course, such that can also be automated easily (i.e., multiple choice questions).\nI came up with the following quiz, which I thought would be nice to share here. I guess that experts should probably be able to complete this in a few minutes, intermediate/beginners would probably complete this by up to 30 minutes.\nExam instructions\rThe following exam contains 10 questions which spans across the different topics regaring tidyverse, and some analysis dilemmas. Each question has four options but only one correct answer. Each correct answer provides you with 10 points.\nYou can use any materials you want including but not limited to: cheat sheets, our course materials, stack overflow, package documentation, running code and seeing what it does.\n\rQuestion 1:\rWhen would you use an R markdown file (.Rmd) versus a script file (.R) to save your work?\nIf I want the relative position of the file retained (so that it is easier to load files from the same directory), I will use an .Rmd file, otherwise I will use a .R file.\rWhen I want a complete documentation of my work in a report I will use a .Rmd. I will use a .R file for debugging and sourcing functions.\rThere is no significant difference between the two formats, and they can be used for the same things interchangably.\rThere is no benefit to using .R script files, the .Rmd format is always superior.\r\r\rQuestion 2:\rLook at the following segments of code.\n# segment 1:\rnew_data \u0026lt;- read.csv(\u0026quot;myfilename.csv\u0026quot;)\r# segment 2:\rnew_data %\u0026gt;% group_by(some_cool_suff) %\u0026gt;% summarize(average = mean(avg_me, na.rm = T)) -\u0026gt; updated_df\r# segment 3:\ravg_var \u0026lt;- mean(new_data$avg_me[!is.na(some_cool_stuff)], na.rm = T)\r# segment 4:\rdata.frame(a = 1:10, b = letters[1:10]) %\u0026gt;% sample_n(3)\r\rWhich segments would you classify as tidyverse syntax?\r(tidyverse syntax = code which uses functions from tidyverse packages, in which there is no function that you can replace to a tidyverse equivalent)\nSegment 1 and segment 3.\rSegment 2 and segment 4.\rSegment 4.\rSegment 2.\r\r\rQuestion 3:\rWhat ggplot2 geoms would you use to generate the following charts?\nFigure 1: not generated with ggplot2, Figure 2: geom_point.\rFigure 1: geom_boxplot, Figure 2: geom_line.\rFigure 1: geom_violin, Figure 2: geom_point.\rFigure 1: geom_boxplot, Figure 2: geom_point + geom_line.\r\r\rQuestion 4:\rWhat is the difference between the matrix and the tibble in the following?\nmatrix(cbind(1:10, letters[1:10], LETTERS[1:10]), ncol = 3)\r## [,1] [,2] [,3]\r## [1,] \u0026quot;1\u0026quot; \u0026quot;a\u0026quot; \u0026quot;A\u0026quot; ## [2,] \u0026quot;2\u0026quot; \u0026quot;b\u0026quot; \u0026quot;B\u0026quot; ## [3,] \u0026quot;3\u0026quot; \u0026quot;c\u0026quot; \u0026quot;C\u0026quot; ## [4,] \u0026quot;4\u0026quot; \u0026quot;d\u0026quot; \u0026quot;D\u0026quot; ## [5,] \u0026quot;5\u0026quot; \u0026quot;e\u0026quot; \u0026quot;E\u0026quot; ## [6,] \u0026quot;6\u0026quot; \u0026quot;f\u0026quot; \u0026quot;F\u0026quot; ## [7,] \u0026quot;7\u0026quot; \u0026quot;g\u0026quot; \u0026quot;G\u0026quot; ## [8,] \u0026quot;8\u0026quot; \u0026quot;h\u0026quot; \u0026quot;H\u0026quot; ## [9,] \u0026quot;9\u0026quot; \u0026quot;i\u0026quot; \u0026quot;I\u0026quot; ## [10,] \u0026quot;10\u0026quot; \u0026quot;j\u0026quot; \u0026quot;J\u0026quot;\rtibble(num = 1:10, sl = letters[1:10], cl = LETTERS[1:10])\r## # A tibble: 10 x 3\r## num sl cl ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;\r## 1 1 a A ## 2 2 b B ## 3 3 c C ## 4 4 d D ## 5 5 e E ## 6 6 f F ## 7 7 g G ## 8 8 h H ## 9 9 i I ## 10 10 j J\rThe tibble has named variables (columns) and the matrix does not name the columns.\rThe tibble retains the original data type and the matrix converts the data types.\rmatrix is a base R function and tibble is a tidyverse function.\rAll of the above.\r\r\rQuestion 5:\rWhat stringr function would you use to simplify the following code?\nsome_string \u0026lt;- c(\u0026quot;How are you today?\u0026quot;, \u0026quot;Is this test ok?\u0026quot;, \u0026quot;You\u0026#39;re already half way in!\u0026quot;)\rmap_chr(some_string, ~paste0(stringi::stri_wrap(., width = 5), collapse = \u0026quot;\\n\u0026quot;))\rstr_count.\rstr_wrap.\rstr_sub.\rNo such function: must use a combination of a stringr and a loop (or a map function).\r\r\rQuestion 6:\rWhat is the difference between contains and one_of?\nBoth are “select helpers”, one_of is used to specify strings which starts with one of the specified expressions, and contains lets you specify the variable names in “non standard evaluation” (unquoted) style.\rcontains selects variables based on the regular expression you feed as an argument. one_of needs you to specify the variable names as strings.\rcontains selects variables which contain the literal string you feed into it. one_of needs you to specify the variables names as strings.\rBoth functions do the same thing with the same arguments.\r\r\rQuestion 7:\rWhen reshaping data with the gather function, what is the meaning of the ... argument?\nSpecify which variables to gather by.\rSpecify which variables not to gather by (using the “-” sign).\rSpecify either a or b.\rProvide variable by which to group the resulting tibble.\r\r\rQuestion 8:\rWhat function would you use to get all the rows in tibble1 which are not in tibble2?\nsetdiff(tibble1, tibble2)\rsetdiff(tibble2, tibble1)\rintersect(tibble1, tibble2)\rsemi_join(tibble1, tibble2)\r\r\rQuestion 9:\rAssume you examine the data which appears in the following scatter plot using per-axis boxplots. Would classify point A as an outlier?\nYes, only accoring to the y-axis.\rYes, only according to the x-axis.\rYes, according to either x-axis or y-axis.\rNo, it will not be classified as an outlier.\r\r\rQuestion 10:\rYou encountered a data set in which all variables are normally distributed with an unequal variance and\runequal expectancy (mean). You wish to run a KMeans clustering to cluster the data. What would you do\ras a preprocessing step?\nScale and center the data using the function scale.\rScale and center the data using min-max scaling and centering.\rEither a or b.\rNothing - since the data is already normally distributed, no scaling or centering is required.\r\r\rBonus question (5 points bonus):\rDid you sign up for R-Bloggers updates? (feed to receive R related news and updates)\nYes (5 points bonus).\rNo, but I’m doing it now (2.5 points bonus).\rNo, and I don’t intend to.\r\rP.S. I’m not getting any benefits from R-bloggers for “advertising” them, I genuinly think it’s a great source to stay updated, and improve your R capabilities.\n\rQuiz answers\rAnswers available in the following gist.\n\r","date":1556452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556452800,"objectID":"020dbfb01905c730d5ee58584a07dfe6","permalink":"/post/2019-04-28-test_your_tidyness/","publishdate":"2019-04-28T12:00:00Z","relpermalink":"/post/2019-04-28-test_your_tidyness/","section":"post","summary":"Over the last month I gave a tidyverse + intro to data science corporate training in a startup in Tel-Aviv. We had two groups (beginners and intermediates), and for the last assignment of the course I was aiming for a short quiz comprised of various topics which we covered during the course, such that can also be automated easily (i.e., multiple choice questions).\nI came up with the following quiz, which I thought would be nice to share here.","tags":["Tidyverse"],"title":"Test your tidyness - a short quiz to check your tidyverse capabilities","type":"post"},{"authors":null,"categories":["R"],"content":"\rA few months ago I attended the 2019 rstudio::conf, including the shiny train-the-trainer workshop. It was a two day workshop and it inspired me in many ways. The first day of the workshop focused on the very basics of teaching (R or anything else), and for me it put the spotlight on things I never considered before.\nOne of the important takeways from the workshop was how to approach educating others: preparing for a course, things you can do during the lessons, and how to self-learn and improve my own teaching methods afterwards.\nThis led me to create the teachR’s cheatsheet. It outlines the basics of teaching and I chose to give it the flavour of R (in the examples and illustrations within the cheatsheet).\nI have contributed it to RStudio’s cheat sheet repo, so you can download it directly from: https://github.com/rstudio/cheatsheets/raw/master/teachR.pdf.\nIn the cheat sheet you will find three segments:\nPreparing a new course / workshop / lesson.\rThings you can do during the lesson itself.\rThings you should do when the course is completed in order to improve your own teaching methods.\r\rI previously blogged about some of the things learned at the train-the-trainer, and not everything made it to the cheat sheet, so if you’re interested you can read more here.\nHere’s an example for some of the things you can find in the cheat sheet.\nDesigning a new course\rThe cheat sheet covers the various steps of designing a course, i.e.:\nPersona analysis of your learners.\rDefining the course’s goals using Bloom’s taxonomy.\rUsing conceptual maps to grasp what the the course should look like and what related terms/materials should appear.\rWriting the final exam, the slides, check-ins and faded examples.\r\rHere are some examples relating to 1-2:\nPersona analysis\rTake a while to understand and characterize your learners: are the novice? advanced? false experts?\nWhat are the learner’s goals from the course? what prior knowledge you can assume (and what not), and do they have any special needs.\nIf end up with too many personas anticipate trouble - it’s hard to accomodate a diverse crowd, what are you going to miss out on?\n\rDefine goals using Bloom’s taxonomy\rBloom’s taxonomy illustrates the levels of learning new concepts or topics.\nThe Vanderbilt University Center for Teaching has a nice illustration for it.\nBlooms Taxonomy\n\rYou can visit the Vanderbilt website for a more thorough explanation about the taxonomy, but suffice it to say that “remember” is the most basic form of acquired knowledge, and the highest levels (at the top of the pyramid) are evaluate and create (being able to evaluate someone else’s work, or create your own noval work).\nIf we translate that to R, “remember” might translate to: “learners will be able to state the main packages in tidyverse and their purpose” versus “create” which in that context would translate to: “learners will be able to contribute to a tidyverse packages or create their own tidy package.” You can see that the first is something you can teach an R beginner but the latter is much more complex and can be mastered by an advanced useR.\nWorking with Bloom’s taxonomy can help you set your goals for the course and also help you set the expectations with the learners of your course.\n\r\rDuring the course\rSome tips I learned at the train-the-trainer workshop, for when you are during the lesson itself.\nSticky notes\rAt the start of the lesson, give each learner three sticky notes (green, red, and blue).\rThe learners put them on their computers according to their progress:\n\rGreen = I’m doing fine / finished teh exercise.\rRed = Something is wrong, I need help.\rBlue = I need a break\r\rIf you see a lot of greens - try to up the pace. If you see a lot of reds, maybe take it easier.\n\rCheck-ins\rTry to set a few check-ins every hour, to evaluate the progress and make sure that the learners are “with you”. You can even use some kind of online surveying tool to turn this into a “game”.\n\r\rAfter the course\rMake sure you debrief properly, and learn from your experience. Use surveys to collect feedback. Also measure the time each chapter really takes you, so you can better estimate the time for each type of lesson.\n\rConclusion\rTeaching can be challenging, but it is also rewarding and fun.\nIt is important to come well prepared, and this cheat sheet can help you checklist what you need to do:\rhttps://github.com/rstudio/cheatsheets/raw/master/teachR.pdf\nTeaching is an iterative process in which you can keep improving each time, if you measure and learn from your mistakes.\n\r","date":1552392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552392000,"objectID":"f3020499e5b9473f9ae7cd815a42bb13","permalink":"/post/2019-03-12-the_teachr_cheat_sheet/","publishdate":"2019-03-12T12:00:00Z","relpermalink":"/post/2019-03-12-the_teachr_cheat_sheet/","section":"post","summary":"A few months ago I attended the 2019 rstudio::conf, including the shiny train-the-trainer workshop. It was a two day workshop and it inspired me in many ways. The first day of the workshop focused on the very basics of teaching (R or anything else), and for me it put the spotlight on things I never considered before.\nOne of the important takeways from the workshop was how to approach educating others: preparing for a course, things you can do during the lessons, and how to self-learn and improve my own teaching methods afterwards.","tags":["Train the Trainer","Teaching"],"title":"The teachR's::cheat sheet","type":"post"},{"authors":null,"categories":["R"],"content":"\rA few days ago I presented at the 9th Israeli class action lawsuit conference. You’re probably asking yourself what would a data scientist do in a room full of lawyers?\nApparently, there is a lot to do… Here’s the story: being in market research, we get a lot of lawyers which are faced with class action lawsuits (either suing or being sued) - and they hire us to conduct research and estimate things like the size of the group for the class action, or the total damages applied on the group.\nThis time, we did something special. we conducted our own survey, with consumers in the general public in Israel. The goal was to rate various ways of getting compensation (after settling a class action lawsuit).\nFor that we used conjoint analysis. Conjoint is where you ask the survey participants a set of questions (five in our case). Each question has a number of alternatives (or packages) to choose from, and these are randomized per respondent. In our case we showed three packages, each package is defined by three parameters relating to how a consumer can get compensation in case of a class action being won:\nPush versus pull - do you have to ask for the compensation or would you get the notification/compensation without asking.\rThe value of the compensation - tested at 4 levels (25, 50, 75, and 100 ILS)\rThe method of delivery - as a complimentary product, a refund at next purchase, bank cheque, or credit card.\r\rThe thing about conjoint analysis is that when you diversify enough, you can then run various models to estimate the weight of each parameter, i.e., using logistic regression.\nThe data is available in the github repo, and the specific data is under the data folder.\n#library(tidyverse)\rclass_action_conjoint \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/adisarid/class-action-IL-survey/master/data/20190130020529-SurveyExport-general_public-conjoint.csv\u0026quot;,\rskip = 1,\rcol_names = c(\u0026quot;Response ID\u0026quot;, \u0026quot;Set Number\u0026quot;, \u0026quot;Card Number\u0026quot;, \u0026quot;compensation_push_pull\u0026quot;, \u0026quot;compensation_amount_ILS\u0026quot;, \u0026quot;compensation_type\u0026quot;,\r\u0026quot;score_selection\u0026quot;))\r## Parsed with column specification:\r## cols(\r## `Response ID` = col_double(),\r## `Set Number` = col_double(),\r## `Card Number` = col_double(),\r## compensation_push_pull = col_character(),\r## compensation_amount_ILS = col_double(),\r## compensation_type = col_character(),\r## score_selection = col_double()\r## )\rglimpse(class_action_conjoint)\r## Observations: 7,020\r## Variables: 7\r## $ `Response ID` \u0026lt;dbl\u0026gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10...\r## $ `Set Number` \u0026lt;dbl\u0026gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5,...\r## $ `Card Number` \u0026lt;dbl\u0026gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1,...\r## $ compensation_push_pull \u0026lt;chr\u0026gt; \u0026quot;push\u0026quot;, \u0026quot;push\u0026quot;, \u0026quot;push\u0026quot;, \u0026quot;push\u0026quot;, \u0026quot;pull\u0026quot;...\r## $ compensation_amount_ILS \u0026lt;dbl\u0026gt; 75, 50, 25, 100, 25, 100, 75, 100, 50,...\r## $ compensation_type \u0026lt;chr\u0026gt; \u0026quot;another_product\u0026quot;, \u0026quot;credit_cart\u0026quot;, \u0026quot;ban...\r## $ score_selection \u0026lt;dbl\u0026gt; 0, 0, 100, 0, 100, 0, 0, 0, 100, 0, 10...\rclass_action_conjoint %\u0026gt;% count(compensation_push_pull)\r## # A tibble: 2 x 2\r## compensation_push_pull n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 pull 3516\r## 2 push 3504\rclass_action_conjoint %\u0026gt;% count(compensation_amount_ILS)\r## # A tibble: 4 x 2\r## compensation_amount_ILS n\r## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 25 1759\r## 2 50 1761\r## 3 75 1752\r## 4 100 1748\rclass_action_conjoint %\u0026gt;% count(compensation_type)\r## # A tibble: 5 x 2\r## compensation_type n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 another_product 1391\r## 2 bank_cheque 1410\r## 3 coupon 1403\r## 4 credit_cart 1415\r## 5 refund_next_purchase 1401\rYou can see that the different options are balanced (they should be - they were selected randomly) and that the number of observations is \\(7,020\\). This is because we had \\(n=468\\) respondents answering the conjoint question groups, each selecting best one out of three, with five such random sets (\\(5*3*468=7020\\)).\nLogistic regression\rThe easiest (and most basic) way to start analyzing the conjoint data is with logistic regression. Note that I’m not endorsing this use of logistic regression in conjoint analysis, because nowadays it has become a standard to compensate for mixed effects (see package lme4). However, for the purposes of this post, I’m going to carry on with the simple glm which is sufficiently good for our illustration. In any case, my experience is that the models yield similar results in most cases.\nglm_set \u0026lt;- class_action_conjoint %\u0026gt;% mutate(score_selection = score_selection/100) %\u0026gt;% mutate(compensation_push_pull = factor(compensation_push_pull,\rlevels = c(\u0026quot;pull\u0026quot;, \u0026quot;push\u0026quot;),\rordered = F),\rcompensation_type = factor(compensation_type,\rlevels = c(\u0026quot;another_product\u0026quot;,\r\u0026quot;refund_next_purchase\u0026quot;,\r\u0026quot;coupon\u0026quot;,\r\u0026quot;bank_cheque\u0026quot;,\r\u0026quot;credit_cart\u0026quot;),\rordered = F)) %\u0026gt;% select(-`Set Number`, -`Card Number`, -`Response ID`) %\u0026gt;% mutate(compensation_amount_ILS = factor(compensation_amount_ILS, levels = c(25, 50, 75, 100)))\rconjoint_glm_model \u0026lt;- glm(data = glm_set %\u0026gt;% select(score_selection, compensation_push_pull, compensation_amount_ILS, compensation_type),\rformula = score_selection ~ .,\rfamily = binomial())\rsummary(conjoint_glm_model)\r## ## Call:\r## glm(formula = score_selection ~ ., family = binomial(), data = glm_set %\u0026gt;% ## select(score_selection, compensation_push_pull, compensation_amount_ILS, ## compensation_type))\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8635 -0.8111 -0.5139 0.8796 2.6557 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|)\r## (Intercept) -3.49667 0.11341 -30.832 \u0026lt;2e-16\r## compensation_push_pullpush 0.74140 0.05807 12.767 \u0026lt;2e-16\r## compensation_amount_ILS50 1.01476 0.09385 10.813 \u0026lt;2e-16\r## compensation_amount_ILS75 1.73623 0.09224 18.823 \u0026lt;2e-16\r## compensation_amount_ILS100 2.40149 0.09281 25.876 \u0026lt;2e-16\r## compensation_typerefund_next_purchase 0.08431 0.10399 0.811 0.418\r## compensation_typecoupon 1.10396 0.09588 11.514 \u0026lt;2e-16\r## compensation_typebank_cheque 1.53888 0.09473 16.245 \u0026lt;2e-16\r## compensation_typecredit_cart 1.89640 0.09586 19.782 \u0026lt;2e-16\r## ## (Intercept) ***\r## compensation_push_pullpush ***\r## compensation_amount_ILS50 ***\r## compensation_amount_ILS75 ***\r## compensation_amount_ILS100 ***\r## compensation_typerefund_next_purchase ## compensation_typecoupon ***\r## compensation_typebank_cheque ***\r## compensation_typecredit_cart ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 8936.7 on 7019 degrees of freedom\r## Residual deviance: 7295.6 on 7011 degrees of freedom\r## AIC: 7313.6\r## ## Number of Fisher Scoring iterations: 4\rNote how most variables (actually all but compensation_typerefund_next_purchase) are significant and with a positive estimate (i.e., odds ratio \u0026gt; 1). This means means that when a certain variable increases, the probability of choosing the package increases, i.e.:\n\rPassively getting the compensation (“Push”) is better than a required act to get the compensation (“pull”) .\rAny sum of money (50, 75, 100) is better than 25, in an increasing odds ratio.\rMost compensation types (credit card payback, bank cheque, coupon) are significantly better than a complimentary product.\r\rNow comes the interesting part: for example, compare the following three packages. Try to guess which one is more attractive:\n\r\rParameter\rPackage 1\rPackage 2\rPackage 3\r\r\r\rPush/Pull\rPull\rPull\rPush\r\rReturn\rCredit\rRefund\rCoupon\r\rPrice\r25\r75\r25\r\r\r\rIt is not that easy to determine between the three. In this situation there is no single strategy which is superior to the others, we can however plot these three packages with the logistic regression response and standard errors. First let’s put them all in a tibble (I also added the best and worst packages).\npackage_comparison \u0026lt;- tribble(\r~package_name, ~compensation_push_pull, ~compensation_amount_ILS, ~compensation_type,\r\u0026quot;pkg1\u0026quot;, \u0026quot;pull\u0026quot;, 25, \u0026quot;credit_cart\u0026quot;,\r\u0026quot;pkg2\u0026quot;, \u0026quot;pull\u0026quot;, 75, \u0026quot;refund_next_purchase\u0026quot;,\r\u0026quot;pkg3\u0026quot;, \u0026quot;push\u0026quot;, 25, \u0026quot;coupon\u0026quot;,\r\u0026quot;worst\u0026quot;, \u0026quot;pull\u0026quot;, 25, \u0026quot;another_product\u0026quot;,\r\u0026quot;best\u0026quot;, \u0026quot;push\u0026quot;, 100, \u0026quot;credit_cart\u0026quot;\r) %\u0026gt;% mutate(compensation_amount_ILS = factor(compensation_amount_ILS)) # need to convert to factor - which is how it is modeled in the glm.\rpredicted_responses \u0026lt;- predict(conjoint_glm_model, newdata = package_comparison, type = \u0026quot;response\u0026quot;, se.fit = T)\r# lets join these together\rpackage_responses \u0026lt;- package_comparison %\u0026gt;% mutate(fit = predicted_responses$fit,\rse.fit = predicted_responses$se.fit)\rpackage_responses\r## # A tibble: 5 x 6\r## package_name compensation_pu~ compensation_am~ compensation_ty~ fit\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 pkg1 pull 25 credit_cart 0.168 ## 2 pkg2 pull 75 refund_next_pur~ 0.158 ## 3 pkg3 push 25 coupon 0.161 ## 4 worst pull 25 another_product 0.0294\r## 5 best push 100 credit_cart 0.824 ## # ... with 1 more variable: se.fit \u0026lt;dbl\u0026gt;\r# P.S. - excuse the \u0026quot;credit_cart\u0026quot; typo (I build the model that way, only then noticed...)\rggplot(package_responses %\u0026gt;% slice(1:3) , aes(x = package_name, y = fit)) + geom_point() +\rgeom_errorbar(aes(ymin = fit - se.fit, ymax = fit + se.fit)) + ggtitle(\u0026quot;Package comparison (packages 1-3)\u0026quot;, subtitle = \u0026quot;Error bars represent the SE\u0026quot;) + ylab(\u0026quot;Predicted response (glm logit)\u0026quot;) + xlab(\u0026quot;Package name\u0026quot;) +\rscale_y_continuous(labels = scales::percent_format(accuracy = 1))\rggplot(package_responses, aes(x = package_name, y = fit)) + geom_point() +\rgeom_errorbar(aes(ymin = fit - se.fit, ymax = fit + se.fit)) + ggtitle(\u0026quot;Package comparison (including best and worst packages)\u0026quot;, subtitle = \u0026quot;Error bars represent the SE\u0026quot;) + ylab(\u0026quot;Predicted response (glm logit)\u0026quot;) +\rxlab(\u0026quot;Package name\u0026quot;) +\rscale_y_continuous(labels = scales::percent_format(accuracy = 1))\rWe see that the three packages (pkg1, pkg2, and pkg3) are relatively similar, within one standard error from one another. When compared to the worst package they are roughly \\(\\sim8\\) times better (via odds ratio), but the best package is \\(\\sim5\\) times better than packages 1-3.\nOne can use these concepts to illustrate the benefits of each parameter on the different packages, and let the user experience how different features make the packages more or less “attractive”.\nAs an experiment, I prepared a nice little shiny app which lets the user experiment with the different features: build two packages and then compare them. You can checkout the code at the github repo, or check out the live app here.\n\rConclusions\rSurveys are a popular tool used in class actions (at least in Israel). They can be used to estimate the tradeoffs between various types of compensation or settlement, for example with the use of conjoint analysis.\nWith a glm model one can tell the differences of various packages, and the odds ratio is a way to illustrate to decision makers a comparison of various options (and how much “more attractive” is one package over another).\nA shiny app can be a nice way to illustrate the results of a conjoint analysis, and to let the user experiment with how different features make a specific option better or worse than another option.\n\r","date":1549195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549195200,"objectID":"afd35131d0c5babf2c2f856fccadaf41","permalink":"/post/2019-02-03-class-action-conjoint/","publishdate":"2019-02-03T12:00:00Z","relpermalink":"/post/2019-02-03-class-action-conjoint/","section":"post","summary":"A few days ago I presented at the 9th Israeli class action lawsuit conference. You’re probably asking yourself what would a data scientist do in a room full of lawyers?\nApparently, there is a lot to do… Here’s the story: being in market research, we get a lot of lawyers which are faced with class action lawsuits (either suing or being sued) - and they hire us to conduct research and estimate things like the size of the group for the class action, or the total damages applied on the group.","tags":["Conjoint analysis","Class actions","shiny"],"title":"Settling class action lawsuits with conjoint analysis and R (+a conjoint shiny app)","type":"post"},{"authors":null,"categories":["R"],"content":"\rWith all the functional programming going on (i.e., purrr::map and the likes), there is at least one thing that I found missing: progress bars. The plyr::do function had a nice looking progress bar open up by default if the operation took more than 2 seconds and had at least two more to go (as per Hadley’s description in Issue#149 in tidyverse/purrr).\nThe issue is still open, for the time of writing these lines, and will probably be solved sometime in the near future as a feature of purrr::map.\nPersonally, I like @cderv’s elegent solution suggested at that same github issue.\nHere is an example implementation for reading multiple files within a directory and combining them into a single tibble while showing a progress bar when reading the files. The file reading is very similar to what was suggested in this post.\nlibrary(purrr)\rlibrary(readr)\rlibrary(dplyr)\r# directory from which to read a bunch of files (the example here uses csv)\rfile_list \u0026lt;- dir(path = \u0026quot;PATH_TO_DIRECTORY\u0026quot;, pattern = \u0026quot;.csv\u0026quot;)\r# define reading function which includes the progress bar updates and printing\rread_with_progress \u0026lt;- function(filename){\rpb$tick()$print()\rdata_read \u0026lt;- read_csv(filename)\r# you can add additional operations on data_read, or # decide on entirely different task that this function should do.\r}\r# create the progress bar with a dplyr function. pb \u0026lt;- progress_estimated(length(file_list))\rres \u0026lt;- file_list %\u0026gt;%\rmap_df(~read_with_progress(.))\rThat’s it. You’re set to go with a cool progress bar which will print out something like this while the operation is carried out:\n|===================================== |80% ~23 s remaining\r","date":1548417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548417600,"objectID":"afa1fe5368f57b1caff3bb0279096363","permalink":"/post/2019-01-24-purrrying-progress-bars/","publishdate":"2019-01-25T12:00:00Z","relpermalink":"/post/2019-01-24-purrrying-progress-bars/","section":"post","summary":"With all the functional programming going on (i.e., purrr::map and the likes), there is at least one thing that I found missing: progress bars. The plyr::do function had a nice looking progress bar open up by default if the operation took more than 2 seconds and had at least two more to go (as per Hadley’s description in Issue#149 in tidyverse/purrr).\nThe issue is still open, for the time of writing these lines, and will probably be solved sometime in the near future as a feature of purrr::map.","tags":["purrr","progress bars"],"title":"Purrring progress bars (adding a progress bar to `purrr::map`)","type":"post"},{"authors":null,"categories":["R"],"content":"\rFollowing Jenny Bryan’s talk on tidyeval in the last rstudio::conf 2019, I decided to write this short note (mainly as a reminder to myself).\nWhat is tidyeval?\rTidy evaluation, or non standard evaluation, allows us to pass column names between functions. This is the “classic” behaviour of most tidyverse functions. For example, we use:\nlibrary(tidyverse)\rmtcars %\u0026gt;% select(mpg, cyl)\r## mpg cyl\r## Mazda RX4 21.0 6\r## Mazda RX4 Wag 21.0 6\r## Datsun 710 22.8 4\r## Hornet 4 Drive 21.4 6\r## Hornet Sportabout 18.7 8\r## Valiant 18.1 6\r## Duster 360 14.3 8\r## Merc 240D 24.4 4\r## Merc 230 22.8 4\r## Merc 280 19.2 6\r## Merc 280C 17.8 6\r## Merc 450SE 16.4 8\r## Merc 450SL 17.3 8\r## Merc 450SLC 15.2 8\r## Cadillac Fleetwood 10.4 8\r## Lincoln Continental 10.4 8\r## Chrysler Imperial 14.7 8\r## Fiat 128 32.4 4\r## Honda Civic 30.4 4\r## Toyota Corolla 33.9 4\r## Toyota Corona 21.5 4\r## Dodge Challenger 15.5 8\r## AMC Javelin 15.2 8\r## Camaro Z28 13.3 8\r## Pontiac Firebird 19.2 8\r## Fiat X1-9 27.3 4\r## Porsche 914-2 26.0 4\r## Lotus Europa 30.4 4\r## Ford Pantera L 15.8 8\r## Ferrari Dino 19.7 6\r## Maserati Bora 15.0 8\r## Volvo 142E 21.4 4\rThe two variables were selected out of the mtcars data set, and we specified them as names without using any quotation marks. They are symbolic, not characters (although they could also be specified as characters, select is smart enough that way).\nBut assume we want to pass variables “tidy style” between functions which do different operations.\n\rVariation one - a basic example\rWe’ll start simple: a function which has two parameters. The first parameter is a dataset. The second parameters is a grouping variable. All other variables in the data set will have their mean computed using summarize_all.\ntest1 \u0026lt;- function(dataset, groupby_vars){\rgrouping_vars \u0026lt;- enquo(groupby_vars)\rdataset %\u0026gt;% group_by(!! grouping_vars) %\u0026gt;%\rsummarize_all(funs(mean(.))) %\u0026gt;%\rreturn()\r}\rmtcars %\u0026gt;%\rselect(cyl:carb) %\u0026gt;%\rtest1(groupby_vars = cyl)\r## # A tibble: 3 x 10\r## cyl disp hp drat wt qsec vs am gear carb\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 4 105. 82.6 4.07 2.29 19.1 0.909 0.727 4.09 1.55\r## 2 6 183. 122. 3.59 3.12 18.0 0.571 0.429 3.86 3.43\r## 3 8 353. 209. 3.23 4.00 16.8 0 0.143 3.29 3.5\rWe can see that mtcars was grouped by cyl which was passed as a name (not characters). The function test1 took it, then enquo()-ed it, and eventually used it in the tidy chain using !!.\rThe function enquo turns the input into a “quosure”. Then the !! “uses” the quosure to select the proper variable from mtcars.\n\rPassing arguments using ...\rA slightly more complex situation is passing multiple arguments to the function. Assume that this time we want to construct a function which gets one input by which to group by, and what are the variables to be summarized:\ntest2 \u0026lt;- function(dataset, groupby_vars, ...){\rgrouping_vars \u0026lt;- enquo(groupby_vars)\rdataset %\u0026gt;% group_by(!! grouping_vars) %\u0026gt;%\rsummarize_at(vars(...), funs(mean(.))) %\u0026gt;%\rreturn()\r}\rmtcars %\u0026gt;%\rselect(cyl:carb) %\u0026gt;%\rtest2(groupby_vars = cyl, disp:drat)\r## # A tibble: 3 x 4\r## cyl disp hp drat\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 4 105. 82.6 4.07\r## 2 6 183. 122. 3.59\r## 3 8 353. 209. 3.23\rWhat happend is that test2 treats the grouping variable the same way that test1 treated it, but it also passed along the variables disp:drat.\n\rMaximum flexibility - multiple enquo()s\rSometime passing the dots, i.e., ... is not enough.\rFor example, if we want specify behaviour for different columns of the data frame (e.g., compute the mean for some and the std for others). In such cases we need a more flexible version. We can extend the flexibilty of this approach using multiple enqou()s.\ntest3 \u0026lt;- function(dataset, groupby_vars, computemean_vars, computestd_vars){\rgrouping_vars \u0026lt;- enquo(groupby_vars)\rmean_vars \u0026lt;- enquo(computemean_vars)\rstd_vars \u0026lt;- enquo(computestd_vars)\rdataset %\u0026gt;% group_by(!! grouping_vars) %\u0026gt;%\rsummarize_at(vars(!!mean_vars), funs(mean(.))) %\u0026gt;%\rleft_join(dataset %\u0026gt;%\rgroup_by(!! grouping_vars) %\u0026gt;%\rsummarize_at(vars(!!std_vars), funs(sd(.))))\r}\rmtcars %\u0026gt;% test3(groupby_vars = cyl, disp:drat, wt:carb)\r## Joining, by = \u0026quot;cyl\u0026quot;\r## # A tibble: 3 x 10\r## cyl disp hp drat wt qsec vs am gear carb\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 4 105. 82.6 4.07 0.570 1.68 0.302 0.467 0.539 0.522\r## 2 6 183. 122. 3.59 0.356 1.71 0.535 0.535 0.690 1.81 ## 3 8 353. 209. 3.23 0.759 1.20 0 0.363 0.726 1.56\rIn the resulting table, the first column cyl is the grouping variable, columns disp through drat have the mean of the corresponding variables, and columns wt through carb have their standard deviation computed.\n\rAdditional uses of tidy evaluation\rThis evaluation is very useful when building flexible functions, but also when using the ggplot2 syntax within functions, and more so when using Shiny applications, in which input parameters need to go in as grouping or as plotting parameters.\nHowever, this is a topic for a different post.\n\rConclusions\rTidy evaluation empowers you with great tools - it offers a great degree of flexibilty, but it’s a bit tricky to master.\nMy suggestion is that if you’re trying to master tidy evaluation, just think about your use case: which of the three variations presented in this post it resembles too?\nWork your way up - from the simplest version (if it works for you) and up to the complex (but most flexible) version.\n\r","date":1547985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547985600,"objectID":"bffee4df110f883415ea043910de9482","permalink":"/post/2019-01-20-short-note-about-tidy-eval/","publishdate":"2019-01-20T12:00:00Z","relpermalink":"/post/2019-01-20-short-note-about-tidy-eval/","section":"post","summary":"Following Jenny Bryan’s talk on tidyeval in the last rstudio::conf 2019, I decided to write this short note (mainly as a reminder to myself).\nWhat is tidyeval?\rTidy evaluation, or non standard evaluation, allows us to pass column names between functions. This is the “classic” behaviour of most tidyverse functions. For example, we use:\nlibrary(tidyverse)\rmtcars %\u0026gt;% select(mpg, cyl)\r## mpg cyl\r## Mazda RX4 21.0 6\r## Mazda RX4 Wag 21.","tags":["tidyeval"],"title":"Short note about tidyeval","type":"post"},{"authors":null,"categories":null,"content":" Adi Sarid I\u0026rsquo;m a Partner and the Head of Operations Research Department at Sarid Research Services (a boutique market research agency). I\u0026rsquo;m also a PhD student at Tel-Aviv university, researching robustness and equity in power grid expansion planning. Very broadly, my interests include:\n Data science, Operations research, Market research.  You can reach out via my linkedin, or simply email me adi@sarid-ins.co.il.\nFor more information about Sarid Research Institute, check out Sarid Research (English site) or Sarid Research (Hebrew site).\n","date":1547899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547899200,"objectID":"6083a88ee3411b0d17ce02d738f69d47","permalink":"/about/","publishdate":"2019-01-19T12:00:00Z","relpermalink":"/about/","section":"","summary":"Adi Sarid I\u0026rsquo;m a Partner and the Head of Operations Research Department at Sarid Research Services (a boutique market research agency). I\u0026rsquo;m also a PhD student at Tel-Aviv university, researching robustness and equity in power grid expansion planning. Very broadly, my interests include:\n Data science, Operations research, Market research.  You can reach out via my linkedin, or simply email me adi@sarid-ins.co.il.\nFor more information about Sarid Research Institute, check out Sarid Research (English site) or Sarid Research (Hebrew site).","tags":null,"title":"About","type":"page"},{"authors":null,"categories":["R"],"content":"\rFirst, let me start by saying wow!, what a wonderful experience.\nWhen I booked the trip from Israel to Austin, TX, I thought that I’ll see some good content, and learn at the conference (as I in fact did). It was much more enjoyable than I could’ve imagined. In part I guess this can be contributed to the awesome R community. The ease in which you start a conversation with just about anyone in the conference - about R, professional life (or even personal life), that’s great.\nBesides that, visiting Texas (for the first time) was interesting, but for more on that - see venue.\nWorkshop “Shiny Train-the-Trainer”\rThis was a two day workshop. The first day was taught by Greg Wilson, and touched different points of teaching in general, and teaching programming lanugages. The second day was taught by Mine, and zoomed-in on Shiny apps and how to teach building Shiny apps.\nStarting from the theory and building up, Greg was very charismatic. For me, some takeaways from the workshop were:\n\rEach time you start a new course, choose 1-2 things you want to improve/tryout. Don’t try to go “all-in” because then you might miss. Also, make sure that when you do implement new techniques, you don’t fall short on things you were doing so far which were good.\r\rThis is going to be slightly cumbersome (I’m also summarizing this for my own good).\nA few things that helps organize and conduct sessions.\nFigure out who are your learners\rFigure out who are your learners: what are they interested in? what do they already know, and what they don’t know. What is the diversity you’re going to get in the crowd (persona analysis).\nHere’s are examples for different personas we cooked up during the workshop.\nA shiny novice\r\rBackground: Statisticians and Data Scientists from the pharma industry.\rPrior knowledge: Some knowledge but not state of the art.\rMotivation: Want to build shiny apps to share information with other functions in the company.\rHow the course will help them: Able to build simple shiny apps and grow from there.\rSpecial needs: “Think they know but actually don’t”.\r\r\rA shiny expert\r\rBackground: New company moving from a start-up. Biostatistics PhD.\rPrior knowledge: Used R, made apps for paper presentations, done some shiny apps. Looks good but they know that there are stuff that they don’t know. Mix of formal (for the statistics) but a lot of fun learning on their own.\rMotivation: Utilize what they have done, but in the context of the organization, and learn about new and cool things.\rHow the course will help them: Teach them the newest and best things of Shiny, in the context of the company. This is how make it production and enterprise ready.\rSpecial needs: Work from home. Interaction remote. Dog that barks. Online course.\r\r\rA student you expect to encounter at a shiny workshop you teach\r\rBackground: Danny is studying industrial engineering. Undergrad. He is in his third year, and about to finish next year. He likes data science and likes aquiring new programming skills.\rPrior knowledge: Danny has learned some python and some base R, throghout the last few semsters, but he is not fluent in either. Still struggling with some commands in R.\rMotivation: Danny wants to aquire a new tools that will help him next year when he looks for new work, and will help him impress potential employers.\rHow the course will help them: Danny will be able to build apps and use them as showcase while he is looking for work next year. In addition, he will be able to build shiny apps that will help him publish and distribute findings.\rSpecial needs: Danny has a lot of motivation, but is a novice to R and programming in general. He doesn’t have a lot of time for exercise because the semster courses take up a lot of his time, nonetheless, he is willing to invest the time in projects at specific “peaks” in order to advance his skills.\r\r\r\rWrite the learning objectives\rWrite learning objectives which are observable (by the learners) and also measureable. For example:\nThe student will build a shiny app that does…\rThe students will learn and use the renderPlot() function, etc.\r\rHere’s another example for learning objectives of a short introductory 1-hour session:\nThe students will understand the basic elements of a shiny app and describe the difference between ui, server and global.\rThe students will apply the princibles to modify an example reading a file and showing a table with the first 10 lines of the file.\rThe students will learn about and be aware of shiny examples in the gallery.\r\rTo formulate the objectives, one can use bloom’s taxonomy (and the extended version of it that Mine have shown during class).\n\rBuild conceptual maps\rBuild concept maps for each of the topics in the course. I.e., for each class there is a concept map that highlights the topics and the connections between them. Again, referring to the steps in Bloom’s taxonomy as the building blocks of those building blocks. While building the concept maps, remmember that:\n\rRoughly 5-9 items can fit in the short term memory. Consider that when building the concept map. Make sure it’s not too complicated.\rTo make things slightly easier, while teaching, you can use a whiteboard to expand the concept map and show the class where we are on the concept map.\r\r\rWrite the “final exam” and formative assesments\rWrite the final exam, i.e., in the end of the session, what should they be able to answer? This should correspond to the aferomentioned learning objectives.\nGenerate formative assesments (short questions for “check-ins”), that will be used during the lesson. These will help you check if the crowd is with you or lost got lost.\n\rCreate the presentation and learning materials\rFinally, create the learning material around the previous steps. Re-iterate as needed to improve the materials.\n\rSome more useful tools and tips\r\rSticky notes are very useful during programming lessons. This way you can see during the lesson where the class is at. Have 4 colors:\r\rGreen = “everything is fine”\rRed = “need help”\rBlue = “want a break”\rOrange = “want to ask a question”\r\rFor “check-ins” You can use poll everywhere or a similar solution.\rInteractions between students are very useful. Ask a question. Let them talk with one another to get the answer.\r“Baby steps” - use faded examples or incremental examples when teaching. Avoid a “novice blank page” when starting.\rEncourage the students: “what would you type into stackoverflow to find a solution to this problem?”\rNo opting-out. If someone doesn’t know the answer, ask someone else - then go back to that person and ask something else. No one will “fall asleep”, you make sure that everyone are with you. Jump between locations in class (not in the sitting order).\rUse rstudio.cloud when a uniform R environment is desired. We can even start from a flat base instance with all the packages pre-loaded.\r\r\rFeedback\r\rChoose an element from above. Use feedback to understand if it was good or not. Choose an existing thing you do and check that you didn’t lose it either.\rEncourage the students to give feedback to one another by interacting in exercises (can scale up to larger classes).\r\r\rFurther reading\rCheck the online materials of the course. Everything is on a creative commons - BY RStudio - license.\nAll resources for teaching techniques are available at Greg Wilson’s website teachtogether.tech.\nResources specific for Shiny teaching are available at the workshop’s website, including Mine’s teaching notes for 1hr, 2hr, 1/2day, 1day, 2day workshops. See http://teach-shiny.rbind.io.\n\r\rConference Day 1\rHere are some highlights:\n\rJoe Chang’s keynote: lots of tools for testing and profiling shiny apps. Speed improvments for shiny apps using cache (plotCacheRender()). Showd some techniques to make apps much quicker.\rAPI development with R and Tensor flow at T-mobile: A really cool use case for using shiny apps and plumber API. This is an example for scaling up a plumber api for a customer facing app.\rDatabases using R: the latest: Edgar demonstrated how he connects to a google big query database. The big query server does all the computations and the clean (and smaller data) is input into R for continued analysis. Very cool.\rWorking with categorial data in R without loosing your mind: some best practices for working with factors. Advocating forecats (actually much of these I already implement in my work anyhow).\rMelt the clock: tidy time series analysis: a talk about tsibble and fable, the packages which are about to replace the forecast and ts(), in a tidy-er version. Can be installed from github and should make life much easier for time series analysis.\r3D mapping, plotting, and printing with rayshader: an extremely cool package for ploting maps from x-y-z (elevation data). A lot of options, and even includes an export for 3d printing of the models.\rgganimate Live Cookbook: A nice package for animating ggplots. But need to carefully choose when to use and when not to use it…\r\r\rConference Day 2\rGreat RMarkdown session - kind of made me rethink about how I do my work. For me personally a lot of the work envolves power point and word, but from now on, I think I’ll try to do it on RMarkdown. More reproduceable, easier to recreate, or update if needed.\n\rYihui Xie talked about blogdown, bookdown, and more recently pagedown (for academic publications). I also came to know about Radix - a package for Academic style blogs. In the new RStudio (version 1.2 and above, currently in preview), there is an option to export RMarkdown documents into power point presentation. Works seamlessly, just change the yaml at the top:\r\r---\rtitle: \u0026quot;This is a power point presentation\u0026quot;\rauthor: \u0026quot;I\u0026#39;m the author\u0026quot;\rdate: \u0026quot;...\u0026quot;\routput: powerpoint_presentation\r---\r\rThe gt package was presented. A package which is great for generating html or \\(\\LaTeX\\) tables.\rA cool feature of RMarkdown which I didn’t know about is parameters. It enables to creat variants of an RMarkdown document without actually changing the document. For more info see parameterized reports.\rHadley Wikam talked about a new package in development vctrs that is supposed to improve type consistency in R. Here’s an example with factor and c’s default behaviour vs. the vec_c solution in vctrs:\r\rc(factor(\u0026quot;a\u0026quot;), factor(\u0026quot;b\u0026quot;))\r## [1] 1 1\rvctrs::vec_c(factor(\u0026quot;a\u0026quot;), factor(\u0026quot;b\u0026quot;))\r## [1] a b\r## Levels: a b\r\rJenny Bryan talked about lazy evaluation. Finally made some sense to me about when to use enquo() and when to use !!.\rAnother good talk about the ipc package for Shiny apps which require heavy and async computations. That package is able to pass queue and interrupt messages between processes.\rDavid Robinson from DataCamp gave a really inspiring talk about what you can and should do openly (hence me writing this blog post, in the hopes that it’ll be the first of many).\r\r\rVenue\rI landed on Monday and the workshops started on Tuesday, so not too much time to hand around, but following the recommendation of the receptionist in my hotel (which was great, Holiday Inn Austin Town Center), I went to Terry Black’s - an original BBQ resteraunt, which was very good (apparantly also very high on TripAdvisor).\nI also checked out the South of Colorado (SoCo) area - it was nice to walk around and by some small things for the wife and kids.\nParks around the Colorado River - real nice to walk around or jog.\n\rTakeaways\r\rUse more RMarkdown and less powerpoint/word.\rLot of tips on how to improve my R courses (which I should implement).\rShiny is extremly powerful, much more than what I’m using today. Should probably find the time to improve my own Shiny building/programming skills.\rDo a lot more blogging with R and blogdown.\r\r\r","date":1547899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547899200,"objectID":"6f8148aadf5bfaf4a5f5fb1d8d604df3","permalink":"/post/2019-01-19-rstudio-conf-recap/","publishdate":"2019-01-19T12:00:00Z","relpermalink":"/post/2019-01-19-rstudio-conf-recap/","section":"post","summary":"First, let me start by saying wow!, what a wonderful experience.\nWhen I booked the trip from Israel to Austin, TX, I thought that I’ll see some good content, and learn at the conference (as I in fact did). It was much more enjoyable than I could’ve imagined. In part I guess this can be contributed to the awesome R community. The ease in which you start a conversation with just about anyone in the conference - about R, professional life (or even personal life), that’s great.","tags":["rstudio::conf","conference"],"title":"Recap: what I learned in rstudio::conf2019","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536440400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536440400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+03:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":[],"categories":null,"content":"\rClick on the Slides button above to view the built-in slides feature.\n\r\rSlides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483221600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483221600,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00+02:00","relpermalink":"/talk/example/","section":"talk","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461704400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461704400,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00+03:00","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461704400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461704400,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00+03:00","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441054800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441054800,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-09-01T00:00:00+03:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372626000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372626000,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00+03:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne\r Two\r Three\r\nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]